from typing import List
from langgraph.graph import StateGraph, END

from .graph import load_and_chunk_papers, embed_and_index, retrieve, generate_answer, update_convstore
from .state import Paper_Study_State
from .embedding_factory import get_embedder
from .llm import get_llm

class PaperChatBot:
    def __init__(self, arxiv_ids: List[str], embedding_config: dict, use_lightrag: bool = False):
        self.arxiv_ids = arxiv_ids
        self.embedder = get_embedder(embedding_config)
        self.llm = get_llm()
        self.use_lightrag = use_lightrag
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–è®ºæ–‡å‘é‡åº“ï¼Œè¯·ç¨å€™...")
        
        # --- åˆå§‹åŒ–æµç¨‹ï¼šæ‰‹åŠ¨æ‰§è¡ŒèŠ‚ç‚¹ä»¥æ„å»ºå‘é‡åº“ ---
        init_temp_state: Paper_Study_State = {
            "thread_id": "main",
            "arXiv_ids": arxiv_ids,
            "query": "",
            "context": [],
            "answer": "",
            "embedder": self.embedder,
            "vectorstore": None,
            "convstore": None,
            "knowledge_graph": None,
            "graph_context": "",
            "use_lightrag": self.use_lightrag,
            "context_retrieved": "",
            "history_retrieved": "",
            "messages": [],
        }
        
        # æ‰‹åŠ¨è°ƒç”¨ load_and_chunk_papers
        load_command = load_and_chunk_papers(init_temp_state)
        if load_command.update:
            init_temp_state.update(load_command.update)
        
        # æ‰‹åŠ¨è°ƒç”¨ embed_and_index
        embed_command = embed_and_index(init_temp_state)
        if embed_command.update:
            init_temp_state.update(embed_command.update)
        
        self.base_vectorstore = init_temp_state["vectorstore"]
        self.convstore = init_temp_state["convstore"]
        self.knowledge_graph = init_temp_state.get("knowledge_graph")

        # --- ä¸»èŠå¤©å›¾å®šä¹‰ ---
        workflow = StateGraph(Paper_Study_State)
        workflow.add_node("retrieve", retrieve)
        workflow.add_node("generate_answer", generate_answer)
        workflow.add_node("update_convstore", update_convstore)

        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "generate_answer")
        workflow.add_edge("generate_answer", "update_convstore")
        workflow.add_edge("update_convstore", END)

        self.graph = workflow.compile()

        # æ„å»ºåˆå§‹æ¶ˆæ¯
        doc_summary = "å¯ç”¨è®ºæ–‡åˆ—è¡¨ï¼š\n"
        for doc in init_temp_state["context"]:
            if doc.metadata.get("type") == "global_context":
                doc_summary = doc.page_content
                break
        
        lightrag_info = ""
        if self.use_lightrag and self.knowledge_graph:
            lightrag_info = f"\nğŸ§  çŸ¥è¯†å›¾è°±å·²å¯ç”¨: {self.knowledge_graph.get_graph_summary()}\n"
        
        self.initial_msg = (
            "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªæ–‡æ¡£èŠå¤©åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›å¸®åŠ©ï¼\n"
            f"{doc_summary}{lightrag_info}\næˆ‘èƒ½ä¸ºæ‚¨æä¾›ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"
        )

    def chat(self, message: str, history: List[List[str]]) -> str:
        current_state: Paper_Study_State = {
            "thread_id": "main",
            "arXiv_ids": self.arxiv_ids,
            "query": message,
            "context": [],
            "answer": "",
            "embedder": self.embedder,
            "vectorstore": self.base_vectorstore,
            "convstore": self.convstore,
            "knowledge_graph": self.knowledge_graph,
            "graph_context": "",
            "use_lightrag": self.use_lightrag,
            "context_retrieved": "",
            "history_retrieved": "",
            "messages": [],
        }

        result = self.graph.invoke(current_state)
        self.convstore = result["convstore"]
        return result["answer"]

    def get_initial_message(self) -> str:
        return self.initial_msg