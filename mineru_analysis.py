import requests
import os
import time
import glob
import json
from dotenv import load_dotenv
import zipfile
import io
import logging

# é…ç½® logger
logger = logging.getLogger("mineru")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("%(asctime)s %(levelname)s - %(message)s")
ch.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(ch)

class MineruProcessor:
    """
    ä¸€ä¸ªç”¨äºå¤„ç†Mineru PDFè§£æä»»åŠ¡çš„å®¢æˆ·ç«¯ã€‚

    è¿™ä¸ªç±»å°è£…äº†ä¸Mineru APIäº¤äº’çš„æ‰€æœ‰æ­¥éª¤ï¼ŒåŒ…æ‹¬ï¼š
    1. ç”³è¯·ä¸Šä¼ URLã€‚
    2. ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼‰ã€‚
    3. è½®è¯¢ç­‰å¾…è§£æç»“æœã€‚
    4. ä¸‹è½½å¹¶è§£å‹ç»“æœï¼Œæœ€ç»ˆä¿å­˜ä¸ºMarkdownæ–‡ä»¶ã€‚

    ç”¨æ³•:
        processor = MineruProcessor(api_key="YOUR_API_KEY")
        processor.process_directory("path/to/dataset", "path/to/output")
    """

    def __init__(self, api_key: str, 
                 base_url: str = "https://mineru.net/api/v4", 
                 batch_size: int = 50, 
                 timeout_seconds: int = 1800, 
                 polling_interval: int = 10):
        """
        åˆå§‹åŒ–Mineruå¤„ç†å™¨ã€‚

        Args:
            api_key (str): ä½ ä»Mineruå®˜ç½‘ç”³è¯·çš„APIå¯†é’¥ã€‚
            base_url (str, optional): APIçš„åŸºç¡€URLã€‚é»˜è®¤ä¸º "https://mineru.net/api/v4"ã€‚
            batch_size (int, optional): æ¯æ¬¡æ‰¹é‡å¤„ç†çš„æ–‡ä»¶æ•°é‡ã€‚é»˜è®¤ä¸º 50ã€‚
            timeout_seconds (int, optional): è½®è¯¢ç­‰å¾…çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 1800ã€‚
            polling_interval (int, optional): æ¯æ¬¡è½®è¯¢çš„é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 10ã€‚
        """
        if not api_key:
            raise ValueError("API apy_key ä¸èƒ½ä¸ºç©ºã€‚")
            
        self.api_key = api_key
        self.base_url = base_url
        self.batch_size = batch_size
        self.timeout_seconds = timeout_seconds
        self.polling_interval = polling_interval
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    def _get_pdf_file_paths(self, root_dir: str) -> list:
        logger.info(f"æ­£åœ¨ä» '{root_dir}' ç›®å½•ä¸­æŸ¥æ‰¾æ‰€æœ‰ .pdf æ–‡ä»¶...")
        file_paths = glob.glob(os.path.join(root_dir, "**", "*.pdf"), recursive=True)
        logger.info(f"æˆåŠŸæ‰¾åˆ° {len(file_paths)} ä¸ªPDFæ–‡ä»¶ã€‚")
        return file_paths

    def _ensure_dir_exists(self, directory: str):
        if not os.path.exists(directory):
            os.makedirs(directory)

    def _get_upload_urls(self, file_paths: list) -> tuple:
        logger.info("--- æ­¥éª¤ 1: æ­£åœ¨å‘Mineru APIç”³è¯·æ–‡ä»¶ä¸Šä¼ URL ---")
        url = f"{self.base_url}/file-urls/batch"
        files_data = [{"name": os.path.basename(p), "is_ocr": True, "data_id": p} for p in file_paths]
        payload = {"enable_formula": True, "language": "ch", "enable_table": True, "files": files_data, "model_version": "vlm"}
        response = requests.post(url, headers=self.headers, json=payload)
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            batch_id = result["data"]["batch_id"]
            file_urls = result["data"]["file_urls"]
            logger.info(f"æˆåŠŸè·å–ä¸Šä¼ URLã€‚Batch ID: {batch_id}")
            return batch_id, file_urls
        else:
            raise Exception(f"ç”³è¯·ä¸Šä¼ URLå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")

    def _upload_files(self, local_files: list, upload_urls: list):
        logger.info("--- æ­¥éª¤ 2: æ­£åœ¨ä¸Šä¼ æ‰€æœ‰PDFæ–‡ä»¶ ---")
        if len(local_files) != len(upload_urls):
            logger.warning(f"æ–‡ä»¶åˆ—è¡¨æ•°é‡({len(local_files)})ä¸URLåˆ—è¡¨æ•°é‡({len(upload_urls)})ä¸åŒ¹é…ï¼Œè·³è¿‡ä¸Šä¼ ã€‚")
            return
        upload_count = 0
        for local_path, upload_url in zip(local_files, upload_urls):
            try:
                with open(local_path, 'rb') as f:
                    upload_response = requests.put(upload_url, data=f)
                    upload_response.raise_for_status()
                    logger.info(f"  - {os.path.basename(local_path)} ... ä¸Šä¼ æˆåŠŸ")
                    upload_count += 1
            except Exception as e:
                logger.error(f"  - {os.path.basename(local_path)} ... ä¸Šä¼ å¤±è´¥! é”™è¯¯: {e}")
        if upload_count == len(local_files):
            logger.info("å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶å‡å·²æˆåŠŸä¸Šä¼ ã€‚")

    def _poll_for_results(self, batch_id: str) -> list:
        logger.info("--- æ­¥éª¤ 3: æ­£åœ¨ç­‰å¾…è§£æç»“æœ (æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´) ---")
        url = f"{self.base_url}/extract-results/batch/{batch_id}"
        start_time = time.time()
        while True:
            if time.time() - start_time > self.timeout_seconds:
                raise TimeoutError("è½®è¯¢è§£æç»“æœè¶…æ—¶ã€‚")
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            result = response.json()
            if result.get("code") == 0:
                data = result.get("data", {})
                top_level_status = data.get("status", "SUCCESS").upper()
                results_list = data.get("extract_result")
                if isinstance(results_list, list):
                    pending_files = [item.get('file_name') for item in results_list if item.get("state") not in ['done']]
                    if not pending_files:
                        logger.info("  - æ‰€æœ‰æ–‡ä»¶çš„çŠ¶æ€å‡å·²å®Œæˆï¼è§£æä»»åŠ¡æˆåŠŸç»“æŸï¼")
                        return results_list
                    else:
                        done_count = len(results_list) - len(pending_files)
                        first_pending_state = next((item.get("state") for item in results_list if item.get("state") not in ['done']), "N/A")
                        logger.info(f"  - æ‰¹æ¬¡å¤„ç†ä¸­: {done_count}/{len(results_list)} ä¸ªæ–‡ä»¶å·²å®Œæˆã€‚ä»åœ¨ç­‰å¾… {len(pending_files)} ä¸ªæ–‡ä»¶... (ä¾‹å¦‚: {pending_files[0]} çŠ¶æ€ä¸º '{first_pending_state}')")
                else:
                    logger.info(f"  - æ‰¹æ¬¡çŠ¶æ€ä¸ºSUCCESSï¼Œä½†å°šæœªè¿”å›æ–‡ä»¶åˆ—è¡¨ï¼Œç»§ç»­ç­‰å¾…...")
            else:
                logger.info(f"  - ä»»åŠ¡ä»åœ¨å¤„ç†ä¸­... (API code: {result.get('code')}, msg: {result.get('msg')})")
            time.sleep(self.polling_interval)

    def _save_results(self, file_results_list: list, output_dir: str, source_dir: str):
        logger.info("--- æ­¥éª¤ 4: æ­£åœ¨ä¸‹è½½ã€è§£å‹å¹¶ä¿å­˜è§£æç»“æœ ---")
        self._ensure_dir_exists(output_dir)
        for file_result in file_results_list:
            original_path = file_result.get('data_id')
            zip_url = file_result.get('full_zip_url')
            state = file_result.get('state')
            if not original_path: continue
            base_name = os.path.basename(original_path)
            if state != 'done' or not zip_url:
                logger.warning(f" æ–‡ä»¶ {base_name} å¤„ç†æœªæˆåŠŸæˆ–æ²¡æœ‰zipé“¾æ¥ï¼ŒçŠ¶æ€: '{state}'ï¼Œå·²è·³è¿‡ã€‚")
                continue
            try:
                relative_path = os.path.relpath(original_path, source_dir)
                relative_md_path = os.path.splitext(relative_path)[0] + ".md"
                output_filepath = os.path.join(output_dir, relative_md_path)
                output_subfolder = os.path.dirname(output_filepath)
                self._ensure_dir_exists(output_subfolder)
                logger.info(f"  - æ­£åœ¨ä¸‹è½½ {base_name} çš„ç»“æœåŒ…...")
                zip_response = requests.get(
                    zip_url,
                    proxies={"http": None, "https": None},
                    timeout=30
                )
                zip_response.raise_for_status()
                zip_in_memory = io.BytesIO(zip_response.content)
                with zipfile.ZipFile(zip_in_memory, 'r') as zf:
                    md_files_in_zip = [f for f in zf.namelist() if f.lower().endswith('.md')]
                    if not md_files_in_zip: continue
                    markdown_filename = md_files_in_zip[0]
                    logger.info(f"  - æ­£åœ¨ä»ZIPä¸­æå–: {markdown_filename}")
                    markdown_content = zf.read(markdown_filename).decode('utf-8')
                    with open(output_filepath, 'w', encoding='utf-8') as f:
                        f.write(markdown_content)
                    logger.info(f"  - å·²æˆåŠŸä¿å­˜è‡³: {output_filepath}")
            except Exception as e:
                logger.error(f" å¤„ç† {base_name} çš„ç»“æœæ—¶å‘ç”Ÿæ„å¤–: {e}")
        logger.info("å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ç»“æœå‡å·²å¤„ç†å®Œæ¯•ã€‚")

    def process_directory(self, source_dir: str, output_dir: str):
        """
        å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶ã€‚

        è¿™æ˜¯è¯¥ç±»çš„ä¸»è¦å…¥å£æ–¹æ³•ã€‚å®ƒä¼šè‡ªåŠ¨å®ŒæˆæŸ¥æ‰¾æ–‡ä»¶ã€åˆ†æ‰¹ã€
        ä¸Šä¼ ã€è½®è¯¢å’Œä¿å­˜ç»“æœçš„å…¨è¿‡ç¨‹ã€‚

        Args:
            source_dir (str): åŒ…å«PDFæ–‡ä»¶çš„æºç›®å½•è·¯å¾„ (ä¾‹å¦‚ "dataset")ã€‚
            output_dir (str): ç”¨äºä¿å­˜Markdownç»“æœçš„è¾“å‡ºç›®å½•è·¯å¾„ (ä¾‹å¦‚ "output_markdown")ã€‚
        """
        try:
            all_files = self._get_pdf_file_paths(source_dir)
            if not all_files:
                return

            total_files = len(all_files)
            total_batches = (total_files + self.batch_size - 1) // self.batch_size
            logger.info(f"\næ–‡ä»¶æ€»æ•°: {total_files}ã€‚å°†åˆ†ä¸º {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š {self.batch_size} ä¸ªæ–‡ä»¶ã€‚")

            for i in range(total_batches):
                start_index = i * self.batch_size
                end_index = start_index + self.batch_size
                file_batch = all_files[start_index:end_index]

                logger.info(f"\n{'='*20} æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_batches} æ‰¹æ–‡ä»¶ ({len(file_batch)}ä¸ª) {'='*20}")

                batch_id, file_urls = self._get_upload_urls(file_batch)
                self._upload_files(file_batch, file_urls)
                final_results = self._poll_for_results(batch_id)
                self._save_results(final_results, output_dir, source_dir)

            logger.info(f"\nğŸ‰ å…¨éƒ¨ {total_batches} æ‰¹ä»»åŠ¡å‡å·²æˆåŠŸå®Œæˆï¼")

        except requests.exceptions.HTTPError as err:
            logger.error(f"\n[é”™è¯¯] HTTPè¯·æ±‚å¤±è´¥: {err.response.status_code} {err.response.text}")
        except requests.exceptions.RequestException as err:
            logger.error(f"\n[é”™è¯¯] ç½‘ç»œè¿æ¥å¤±è´¥: {err}")
        except Exception as err:
            logger.error(f"\n[é”™è¯¯] ç¨‹åºæ‰§è¡Œå‡ºé”™: {err}")

if __name__ == "__main__":
    load_dotenv()
    
    MINERU_API_KEY = os.getenv("MINERU_API_KEY")

    if not MINERU_API_KEY:
        logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­å·²è®¾ç½® MINERU_API_KEYã€‚")
    else:
        # 1. å®šä¹‰è¾“å…¥å’Œè¾“å‡ºç›®å½•
        pdf_source_directory = r"data\inputs"
        markdown_output_directory = r"data\outputs"

        # 2. åˆ›å»º MineruProcessor å®ä¾‹
        # å¯ä»¥åœ¨è¿™é‡Œè‡ªå®šä¹‰å‚æ•°ï¼Œä¾‹å¦‚ processor = MineruProcessor(api_key=MINERU_API_KEY, batch_size=20)
        processor = MineruProcessor(api_key=MINERU_API_KEY)

        # 3. è°ƒç”¨ä¸»æ–¹æ³•ï¼Œå¯åŠ¨å¤„ç†æµç¨‹
        processor.process_directory(source_dir=pdf_source_directory, output_dir=markdown_output_directory)