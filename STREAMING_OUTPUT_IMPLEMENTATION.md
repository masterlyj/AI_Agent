# æµå¼è¾“å‡ºåŠŸèƒ½å®ç°è¯´æ˜

## æ¦‚è¿°
æœ¬æ¬¡æ›´æ–°ä¸ºRAGç³»ç»Ÿçš„é—®ç­”åŠŸèƒ½æ·»åŠ äº†æµå¼è¾“å‡ºæ”¯æŒï¼Œä½¿å¤§æ¨¡å‹çš„å›ç­”èƒ½å¤ŸåƒChatGPTä¸€æ ·é€å­—æ˜¾ç¤ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ä¿®æ”¹æ–‡ä»¶æ¸…å•

### 1. `src/Knowledge_Graph_Agent/nodes.py`
**æ–°å¢åŠŸèƒ½ï¼š**
- æ·»åŠ äº† `generate_answer_stream()` å¼‚æ­¥ç”Ÿæˆå™¨æ–¹æ³•ï¼Œæ”¯æŒæµå¼ç”Ÿæˆç­”æ¡ˆ
- æ·»åŠ äº† `_build_context()` è¾…åŠ©æ–¹æ³•ï¼Œç”¨äºæ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆä¾›æµå¼å’Œéæµå¼ç‰ˆæœ¬å…±ç”¨ï¼‰

**å…³é”®å®ç°ï¼š**
```python
async def generate_answer_stream(self, state: QueryState):
    """æµå¼ç”Ÿæˆç­”æ¡ˆçš„å¼‚æ­¥ç”Ÿæˆå™¨"""
    # æ„å»ºä¸Šä¸‹æ–‡
    full_context = self._build_context(...)
    
    # å…ˆyieldä¸Šä¸‹æ–‡ä¿¡æ¯
    yield {"type": "context", "context": {...}}
    
    # ä½¿ç”¨LLMçš„æµå¼API
    async for chunk in llm.astream(messages):
        yield {"type": "answer_chunk", "content": chunk.content}
    
    # æ ‡è®°å®Œæˆ
    yield {"type": "answer_chunk", "done": True, "full_answer": full_answer}
```

### 2. `src/Knowledge_Graph_Agent/agent.py`
**æ–°å¢åŠŸèƒ½ï¼š**
- æ·»åŠ äº† `query_stream()` å¼‚æ­¥ç”Ÿæˆå™¨æ–¹æ³•ï¼Œæä¾›æµå¼æŸ¥è¯¢æ¥å£
- ä¿ç•™åŸæœ‰çš„ `query()` æ–¹æ³•ï¼Œä¿è¯å‘åå…¼å®¹

**å…³é”®å®ç°ï¼š**
```python
async def query_stream(self, question: str, ...):
    """é€šè¿‡æµå¼è¾“å‡ºæŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰"""
    # æ­¥éª¤1: æ‰§è¡Œæ£€ç´¢å’Œç²¾æ’
    yield {"type": "status", "content": "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."}
    
    # æ­¥éª¤2: æµå¼ç”Ÿæˆç­”æ¡ˆ
    async for chunk in self.nodes.generate_answer_stream(state):
        yield chunk
    
    # æ­¥éª¤3: è¿”å›å®ŒæˆçŠ¶æ€
    yield {"type": "complete", "answer": full_answer, ...}
```

### 3. `src/Knowledge_Graph_Agent/insurance_rag_gradio.py`
**ä¿®æ”¹åŠŸèƒ½ï¼š**
- é‡å†™äº† `query_knowledge_async()` å‡½æ•°ï¼Œä½¿å…¶æ”¯æŒæµå¼è¾“å‡º
- å®æ—¶æ›´æ–°èŠå¤©å†å²æ˜¾ç¤ºï¼Œé€å­—æ˜¾ç¤ºAIå›ç­”

**å…³é”®å®ç°ï¼š**
```python
async def query_knowledge_async(...):
    """å¼‚æ­¥æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰"""
    accumulated_answer = ""
    
    async for chunk in agent_instance.query_stream(...):
        if chunk_type == "answer_chunk":
            # ç´¯ç§¯ç­”æ¡ˆç‰‡æ®µ
            accumulated_answer += chunk.get("content", "")
            
            # å®æ—¶æ›´æ–°æ˜¾ç¤º
            current_chat = display_chat_history + [
                {"role": "assistant", "content": accumulated_answer}
            ]
            yield current_chat, metrics, ...
```

## æµå¼è¾“å‡ºæµç¨‹

```
ç”¨æˆ·æé—®
   â†“
æ˜¾ç¤º"æ­£åœ¨æ£€ç´¢..."
   â†“
æ‰§è¡Œæ£€ç´¢ â†’ yield çŠ¶æ€æ›´æ–°
   â†“
æ‰§è¡Œç²¾æ’ â†’ yield çŠ¶æ€æ›´æ–°
   â†“
ç”Ÿæˆä¸Šä¸‹æ–‡ â†’ yield ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆæ›´æ–°çŸ¥è¯†å›¾è°±å’Œæ–‡æ¡£æ˜¾ç¤ºï¼‰
   â†“
æµå¼ç”Ÿæˆç­”æ¡ˆ â†’ é€å­—yieldæ¯ä¸ªtokenï¼ˆå®æ—¶æ›´æ–°èŠå¤©ç•Œé¢ï¼‰
   â†“
å®Œæˆ â†’ yield æœ€ç»ˆç»“æœï¼ˆæ›´æ–°å¯¹è¯å†å²ï¼‰
```

## æ•°æ®æµæ ¼å¼

### æµå¼è¾“å‡ºçš„chunkç±»å‹ï¼š

1. **status** - çŠ¶æ€æ›´æ–°
```python
{"type": "status", "content": "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."}
```

2. **context** - ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆåŒ…å«å®ä½“ã€å…³ç³»ã€æ–‡æ¡£ï¼‰
```python
{
    "type": "context",
    "context": {
        "entities": [...],
        "relationships": [...],
        "documents": [...],
        "raw_context": "..."
    }
}
```

3. **answer_chunk** - ç­”æ¡ˆç‰‡æ®µ
```python
{"type": "answer_chunk", "content": "æ ¹æ®", "done": False}
{"type": "answer_chunk", "content": "ä¿é™©", "done": False}
...
{"type": "answer_chunk", "done": True, "full_answer": "å®Œæ•´ç­”æ¡ˆ"}
```

4. **complete** - å®Œæˆä¿¡å·
```python
{
    "type": "complete",
    "answer": "å®Œæ•´ç­”æ¡ˆ",
    "context": {...},
    "chat_history": [...]
}
```

5. **error** - é”™è¯¯ä¿¡æ¯
```python
{"type": "error", "content": "é”™è¯¯æè¿°"}
```

## å‘åå…¼å®¹æ€§

- åŸæœ‰çš„ `agent.query()` æ–¹æ³•ä¿æŒä¸å˜ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
- åªæœ‰Gradioç•Œé¢ä½¿ç”¨äº†æ–°çš„ `agent.query_stream()` æ–¹æ³•
- å¦‚æœéœ€è¦ï¼Œå¯ä»¥éšæ—¶åˆ‡æ¢å›éæµå¼ç‰ˆæœ¬

## ä½¿ç”¨ç¤ºä¾‹

### åœ¨Gradioç•Œé¢ä¸­ï¼ˆå·²è‡ªåŠ¨åº”ç”¨ï¼‰ï¼š
ç”¨æˆ·åœ¨ç•Œé¢è¾“å…¥é—®é¢˜åï¼Œä¼šçœ‹åˆ°ï¼š
1. âœ… ç”¨æˆ·é—®é¢˜ç«‹å³æ˜¾ç¤º
2. ğŸ”„ "æ­£åœ¨æ£€ç´¢..."çŠ¶æ€æç¤º
3. ğŸ“Š çŸ¥è¯†å›¾è°±å’Œæ–‡æ¡£å¯è§†åŒ–é¦–å…ˆåŠ è½½
4. ğŸ’¬ AIå›ç­”é€å­—æ˜¾ç¤ºï¼ˆæµå¼è¾“å‡ºï¼‰
5. âœ… å®Œæˆåä¿å­˜åˆ°å¯¹è¯å†å²

### ç¼–ç¨‹è°ƒç”¨æµå¼APIï¼š
```python
# æµå¼æŸ¥è¯¢
async for chunk in agent.query_stream(
    question="ä»€ä¹ˆæƒ…å†µä¸‹ä¿é™©å…¬å¸ä¼šè±å…ä¿é™©è´¹?",
    mode="hybrid",
    enable_rerank=True
):
    if chunk["type"] == "answer_chunk" and not chunk.get("done"):
        print(chunk["content"], end="", flush=True)
    elif chunk["type"] == "complete":
        print("\nå®Œæˆ!")
```

### éæµå¼è°ƒç”¨ï¼ˆåŸæœ‰æ–¹å¼ï¼‰ï¼š
```python
# ä¼ ç»ŸæŸ¥è¯¢
result = await agent.query(
    question="ä»€ä¹ˆæƒ…å†µä¸‹ä¿é™©å…¬å¸ä¼šè±å…ä¿é™©è´¹?",
    mode="hybrid",
    enable_rerank=True
)
print(result["answer"])
```

## æŠ€æœ¯è¦ç‚¹

1. **å¼‚æ­¥ç”Ÿæˆå™¨**ï¼šä½¿ç”¨ `async for` å®ç°æµå¼æ•°æ®ä¼ è¾“
2. **LangChainæµå¼API**ï¼šè°ƒç”¨ `llm.astream()` è€Œä¸æ˜¯ `llm.ainvoke()`
3. **Gradioæ”¯æŒ**ï¼šGradioçš„ç”Ÿæˆå™¨å‡½æ•°è‡ªåŠ¨æ”¯æŒæµå¼æ›´æ–°UI
4. **çŠ¶æ€ç®¡ç†**ï¼šæ­£ç¡®å¤„ç†å¯¹è¯å†å²çš„ç´¯ç§¯å’Œæ›´æ–°

## æ€§èƒ½ä¼˜åŠ¿

- âœ… ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿé™ä½ï¼šæ— éœ€ç­‰å¾…å®Œæ•´ç­”æ¡ˆç”Ÿæˆå³å¯å¼€å§‹é˜…è¯»
- âœ… æ›´å¥½çš„äº¤äº’ä½“éªŒï¼šç±»ä¼¼ChatGPTçš„æ‰“å­—æœºæ•ˆæœ
- âœ… é•¿ç­”æ¡ˆå‹å¥½ï¼šå³ä½¿ç”Ÿæˆå¾ˆé•¿çš„å›ç­”ä¹Ÿä¸ä¼šè®©ç”¨æˆ·ç­‰å¾…å¤ªä¹…
- âœ… å®æ—¶åé¦ˆï¼šç”¨æˆ·å¯ä»¥çœ‹åˆ°ç³»ç»Ÿæ­£åœ¨å·¥ä½œ

## æµ‹è¯•å»ºè®®

1. å¯åŠ¨Gradioç•Œé¢ï¼š
```bash
python src/Knowledge_Graph_Agent/insurance_rag_gradio.py
```

2. è®¿é—® http://127.0.0.1:7860

3. ä¸Šä¼ æ–‡æ¡£å¹¶ç´¢å¼•

4. æé—®æµ‹è¯•æµå¼è¾“å‡ºæ•ˆæœ

## æ³¨æ„äº‹é¡¹

- æµå¼è¾“å‡ºä¾èµ–LangChain LLMçš„ `astream()` æ–¹æ³•æ”¯æŒ
- ç¡®ä¿ä½¿ç”¨çš„LLMæ¨¡å‹æ”¯æŒæµå¼è¾“å‡ºï¼ˆå¤§éƒ¨åˆ†ä¸»æµæ¨¡å‹éƒ½æ”¯æŒï¼‰
- ç½‘ç»œä¸ç¨³å®šå¯èƒ½å¯¼è‡´æµå¼è¾“å‡ºä¸­æ–­ï¼Œå·²æ·»åŠ å¼‚å¸¸å¤„ç†

## åç»­ä¼˜åŒ–å»ºè®®

1. æ·»åŠ æ‰“å­—é€Ÿåº¦æ§åˆ¶ï¼ˆå¯é€‰çš„å»¶è¿Ÿå‚æ•°ï¼‰
2. æ”¯æŒä¸­æ–­ç”Ÿæˆï¼ˆç”¨æˆ·ç‚¹å‡»åœæ­¢æŒ‰é’®ï¼‰
3. æ·»åŠ æµå¼è¾“å‡ºçš„æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
4. ä¼˜åŒ–é•¿æ–‡æœ¬çš„åˆ†å—ç­–ç•¥

---

**ä¿®æ”¹æ—¥æœŸ**: 2025-11-06
**ç‰ˆæœ¬**: v1.0.0
**çŠ¶æ€**: âœ… å·²å®Œæˆå¹¶æµ‹è¯•é€šè¿‡

