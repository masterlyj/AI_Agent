import os
from typing import List, Optional
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

from .graph import load_and_chunk_papers, embed_and_index, retrieve, generate_answer, update_convstore
from .state import Paper_Study_State
from .embedding_factory import get_embedder
from .llm import get_llm
from .graph import retrieve, rerank, generate_answer # å¯¼å…¥æ–°èŠ‚ç‚¹
from .reranker import RerankerModel # å¯¼å…¥ RerankerModel å’Œé…ç½®

load_dotenv()


class PaperChatBot:
    def __init__(self, arxiv_ids: List[str], embedding_config: Optional[dict] = None, rerank_config: Optional[dict] = None):
        """
        åˆå§‹åŒ–è®ºæ–‡èŠå¤©æœºå™¨äºº
        
        Args:
            arxiv_ids: arXiv è®ºæ–‡ ID åˆ—è¡¨
            embedding_config: Embedding é…ç½®ï¼ˆå¯é€‰ï¼Œæœªæä¾›æ—¶ä» .env è¯»å–ï¼‰
            rerank_config: Rerank é…ç½®ï¼ˆå¯é€‰ï¼Œæœªæä¾›æ—¶ä» .env è¯»å–ï¼‰
        """
        self.arxiv_ids = arxiv_ids
        
        # === Embedding: ä¼˜å…ˆä½¿ç”¨å…¥å‚ï¼›å¦åˆ™ä» .env æ„å»º ===
        if not embedding_config:
            etype = os.getenv("EMBEDDING_TYPE", "ollama").strip()
            
            if etype == "hf":
                model_name = os.getenv("HF_EMBEDDING_MODEL_NAME", "BAAI/bge-m3").strip()
                model_kwargs = {}
                device = os.getenv("HF_EMBEDDING_DEVICE", "").strip()
                if device:
                    model_kwargs["device"] = device
                if os.getenv("HF_EMBEDDING_TRUST_REMOTE_CODE", "false").lower() == "true":
                    model_kwargs["trust_remote_code"] = True
                
                embedding_config = {
                    "type": "hf",
                    "model_name": model_name,
                    "model_kwargs": model_kwargs,
                    "encode_kwargs": {},
                    "show_progress": os.getenv("HF_EMBEDDING_SHOW_PROGRESS", "false").lower() == "true",
                    "multi_process": os.getenv("HF_EMBEDDING_MULTI_PROCESS", "false").lower() == "true",
                }
                print(f"âœ… é…ç½® HuggingFace Embedding: {model_name}")
            
            elif etype == "ollama":
                embedding_config = {
                    "type": "ollama",
                    "model": os.getenv("OLLAMA_EMBEDDING_MODEL", "qwen3-embedding:0.6b").strip(),
                    "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").strip(),
                }
                print(f"âœ… é…ç½® Ollama Embedding: {embedding_config['model']}")
            
            elif etype == "vllm":
                base_url = os.getenv("VLLM_BASE_URL")
                if not base_url:
                    raise ValueError("EMBEDDING_TYPE=vllm ä½†æœªé…ç½® VLLM_BASE_URL")
                embedding_config = {
                    "type": "vllm",
                    "model": os.getenv("VLLM_EMBEDDING_MODEL", "text-embedding-3-large").strip(),
                    "base_url": base_url,
                    "api_key": os.getenv("VLLM_API_KEY", "EMPTY"),
                }
                print(f"âœ… é…ç½® vLLM Embedding: {embedding_config['model']}")
            else:
                raise ValueError(f"æœªçŸ¥ EMBEDDING_TYPE: {etype}")
        
        self.embedder = get_embedder(embedding_config)
        self.llm = get_llm()
        
        # === Rerank: ä¼˜å…ˆç”¨å…¥å‚ï¼›å¦åˆ™çœ‹ .env ===
        self.reranker = None
        env_enabled = os.getenv("RERANK_ENABLED", "false").lower() == "true"
        cfg = rerank_config or {}
        enabled = cfg.get("enabled", env_enabled)
        
        if enabled:
            print("ğŸš€ æ­£åœ¨åŠ è½½ Reranker æ¨¡å‹...")
            model = cfg.get("model", os.getenv("RERANK_MODEL", "maidalun1020/bce-reranker-base_v1").strip())
            device = cfg.get("device", os.getenv("RERANK_DEVICE", "").strip() or None)
            top_k = int(cfg.get("top_k", os.getenv("RERANK_TOP_K", "20")))
            use_fp16 = cfg.get("use_fp16", os.getenv("RERANK_USE_FP16", "false").lower() == "true")
            
            self.reranker = RerankerModel(
                model_name_or_path=model,
                device=device,
                top_k=top_k,
                use_fp16=use_fp16
            )
            print(f"âœ… Reranker æ¨¡å‹åŠ è½½å®Œæˆ (model={model}, top_k={top_k})")
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–è®ºæ–‡å‘é‡åº“ï¼Œè¯·ç¨å€™...")
        
        # --- åˆå§‹åŒ–æµç¨‹ï¼šæ‰‹åŠ¨æ‰§è¡ŒèŠ‚ç‚¹ä»¥æ„å»ºå‘é‡åº“ ---
        init_temp_state: Paper_Study_State = {
            "arXiv_ids": arxiv_ids,
            "query": "",
            "context": [],
            "embedder": self.embedder,
            "vectorstore": None,
            "convstore": None,
            "context_retrieved": "",
            "history_retrieved": "",
            "answer": "",
            "messages": [],
        }
        
        # æ‰‹åŠ¨è°ƒç”¨ load_and_chunk_papers
        load_command = load_and_chunk_papers(init_temp_state)
        if load_command.update:
            init_temp_state.update(load_command.update)
        
        # æ‰‹åŠ¨è°ƒç”¨ embed_and_index
        embed_command = embed_and_index(init_temp_state)
        if embed_command.update:
            init_temp_state.update(embed_command.update)
        
        self.base_vectorstore = init_temp_state["vectorstore"]
        self.convstore = init_temp_state["convstore"]

        # --- ä¸»èŠå¤©å›¾å®šä¹‰ ---
        workflow = StateGraph(Paper_Study_State)
        workflow.add_node("retrieve", retrieve)
        workflow.add_node("rerank", rerank) # æ–°å¢ rerank èŠ‚ç‚¹
        workflow.add_node("generate_answer", generate_answer)
        workflow.add_node("update_convstore", update_convstore)

        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "rerank")
        workflow.add_edge("rerank", "generate_answer")
        workflow.add_edge("generate_answer", "update_convstore")
        workflow.add_edge("update_convstore", END)

        self.graph = workflow.compile()

        # æ„å»ºåˆå§‹æ¶ˆæ¯
        doc_summary = "å¯ç”¨è®ºæ–‡åˆ—è¡¨ï¼š\n"
        for doc in init_temp_state["context"]:
            if doc.metadata.get("type") == "global_context":
                doc_summary = doc.page_content
                break
        self.initial_msg = (
            "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªæ–‡æ¡£èŠå¤©åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›å¸®åŠ©ï¼\n"
            f"{doc_summary}\n\næˆ‘èƒ½ä¸ºæ‚¨æä¾›ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"
        )

    def chat(self, message: str, history: List[List[str]]) -> str:
        current_state: Paper_Study_State = {
            "arXiv_ids": self.arxiv_ids,
            "query": message,
            "context": [],
            "embedder": self.embedder,
            "vectorstore": self.base_vectorstore,
            "convstore": self.convstore,
            "context_retrieved": "",
            "history_retrieved": "",
            "answer": "",
            "messages": [],
            "reranker": self.reranker,  # ä¼ é€’ Reranker æ¨¡å‹å®ä¾‹
        }

        result = self.graph.invoke(current_state)
        self.convstore = result["convstore"]
        return result["answer"]

    def get_initial_message(self) -> str:
        return self.initial_msg