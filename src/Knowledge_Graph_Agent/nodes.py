from typing import Dict, Any, TypedDict, Literal, Optional, List
#æ–°å¢Documentå¯¼å…¥
from langchain_core.documents import Document
from .light_graph_rag import LightRAG
from .state import IndexingState, QueryState
from .utils import logger
from .base import QueryParam

class WorkflowNodes:
    def __init__(self, rag_instance: LightRAG):
        self.rag = rag_instance

    # === Indexing: å•ä¸€èŠ‚ç‚¹è°ƒç”¨ ainsert ===
    async def index_documents(self, state: IndexingState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:è§¦å‘ LightRAG çš„å®Œæ•´ç´¢å¼•æµç¨‹ã€‚
        å°†å¤æ‚é˜Ÿåˆ—é€»è¾‘å°è£…åœ¨ ainsert å†…éƒ¨,LangGraph åªè´Ÿè´£å¯åŠ¨å’Œç›‘æ§ã€‚
        """
        logger.info("--- æ­£åœ¨è¿è¡ŒèŠ‚ç‚¹ï¼šindex_documents ---")
        try:
            track_id = await self.rag.ainsert(
                input=state["inputs"],
                ids=state.get("ids"),
                file_paths=state.get("file_paths")
            )
            logger.info(f"âœ… ç´¢å¼•ä»»åŠ¡å·²æäº¤,Track ID: {track_id}")
            return {
                "track_id": track_id,
                "status_message": "Document indexing started successfully."
            }
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return {
                "track_id": None,
                "status_message": f"Indexing failed: {str(e)}"
            }

    # === Querying ===
    async def retrieve_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:ä»çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šretrieve_context ---")
        try:
            query = state["query"]
            query_mode = state.get("query_mode", "hybrid")
            
            # è°ƒç”¨ LightRAG çš„ aquery_data æ–¹æ³•ï¼Œå®ƒåªæ£€ç´¢æ•°æ®è€Œä¸è°ƒç”¨ LLM
            # æˆ‘ä»¬æ£€ç´¢æ›´å¤šçš„æ–‡æ¡£ï¼ˆä¾‹å¦‚ 20 ä¸ªï¼‰ä»¥ä¾›ç²¾æ’
            logger.info(f"æ­£åœ¨ä»¥ '{query_mode}' æ¨¡å¼ä¸ºæŸ¥è¯¢è¿›è¡Œç²—æ’æ£€ç´¢...")
            retrieval_result = await self.rag.aquery_data(
                query,
                param=QueryParam(mode=query_mode, chunk_top_k=20)
            )
            
            # ä»è¿”å›çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–æ‰€æœ‰ä¿¡æ¯
            data = retrieval_result.get("data", {})
            retrieved_chunks_data = data.get("chunks", [])
            retrieved_entities = data.get("entities", [])
            retrieved_relationships = data.get("relationships", [])
            
            # å°†å­—å…¸æ ¼å¼çš„ chunks è½¬æ¢ä¸º LangChain çš„ Document å¯¹è±¡ï¼Œä»¥ä¾¿åç»­å¤„ç†
            retrieved_docs = [
                Document(
                    page_content=chunk.get("content", ""),
                    metadata={
                        "file_path": chunk.get("file_path"),
                        "chunk_id": chunk.get("chunk_id"),
                        "reference_id": chunk.get("reference_id"),
                    }
                ) for chunk in retrieved_chunks_data
            ]
            
            logger.info(f"âœ… ç²—æ’æ£€ç´¢å®Œæˆ:")
            logger.info(f"   - æ–‡æ¡£å—: {len(retrieved_docs)} ä¸ª")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            
            # # æ‰“å°å®ä½“ä¿¡æ¯
            # if retrieved_entities:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ“Š æ£€ç´¢åˆ°çš„å®ä½“")
            #     logger.info("=" * 60)
            #     for idx, entity in enumerate(retrieved_entities[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            #         logger.info(f"  [{idx}] {entity.get('entity_name', 'æœªçŸ¥')}")
            #         logger.info(f"      ç±»å‹: {entity.get('entity_type', 'æœªçŸ¥')}")
            #         logger.info(f"      æè¿°: {entity.get('description', 'æ— ')[:100]}")
            #     if len(retrieved_entities) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_entities) - 5} ä¸ªå®ä½“")
            #     logger.info("=" * 60 + "\n")
            
            # # æ‰“å°å…³ç³»ä¿¡æ¯
            # if retrieved_relationships:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ”— æ£€ç´¢åˆ°çš„å…³ç³»")
            #     logger.info("=" * 60)
            #     for idx, rel in enumerate(retrieved_relationships[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
            #         logger.info(f"  [{idx}] {rel.get('src_id', '?')} â†’ {rel.get('tgt_id', '?')}")
            #         logger.info(f"      å…³ç³»: {rel.get('description', 'æ— ')[:100]}")
            #         logger.info(f"      æƒé‡: {rel.get('weight', 0):.2f}")
            #     if len(retrieved_relationships) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_relationships) - 5} æ¡å…³ç³»")
            #     logger.info("=" * 60 + "\n")
            
            # å°†åŸå§‹æ–‡æ¡£åˆ—è¡¨å’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯æ”¾å…¥ stateï¼Œä¼ é€’ç»™ rerank èŠ‚ç‚¹
            return {
                "retrieved_docs": retrieved_docs,
                "retrieved_entities": retrieved_entities,
                "retrieved_relationships": retrieved_relationships
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {e}")
            return {
                "retrieved_docs": [],
                "retrieved_entities": [],
                "retrieved_relationships": []
            }
        
    async def rerank_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: ä½¿ç”¨ BCE Reranker å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œç²¾æ’ï¼Œå¹¶æ‰“å°åˆ†æ•°ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šrerank_context (ç²¾æ’) ---")

        # # ğŸ”§ è¯¦ç»†è°ƒè¯•æ—¥å¿—
        # logger.info(f"ğŸ“¦ ç²¾æ’èŠ‚ç‚¹æ¥æ”¶åˆ°çš„ State ä¿¡æ¯:")
        # logger.info(f"   - State keys: {list(state.keys())}")
        # logger.info(f"   - 'reranker' in state: {'reranker' in state}")
        # logger.info(f"   - state.get('reranker'): {state.get('reranker')}")
        # logger.info(f"   - type(state.get('reranker')): {type(state.get('reranker'))}")
        # logger.info(f"   - 'retrieved_docs' in state: {'retrieved_docs' in state}")
        # logger.info(f"   - retrieved_docs æ•°é‡: {len(state.get('retrieved_docs', []))}")

        reranker = state.get("reranker")
        docs_to_rerank = state.get("retrieved_docs", [])

        # ğŸ”§ å¢å¼ºåˆ¤æ–­é€»è¾‘
        if reranker is None:
            logger.warning("âš ï¸ Reranker æœªé…ç½® (state['reranker'] is None)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": docs_to_rerank}  # ç›´æ¥å°†åŸå§‹æ–‡æ¡£ä¼ é€’ä¸‹å»
        
        if not docs_to_rerank:
            logger.warning("âš ï¸ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ (retrieved_docs ä¸ºç©º)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": []}

        try:
            query = state["query"]
            passages = [doc.page_content for doc in docs_to_rerank]
            
            logger.info(f"ğŸ¯ å¼€å§‹ç²¾æ’: å¯¹ {len(passages)} ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ–°æ’åº...")
            
            # è°ƒç”¨ reranker æ¨¡å‹è¿›è¡Œè®¡ç®—
            results = reranker.rerank(query, passages)
            rerank_ids = results.get('rerank_ids', [])
            rerank_scores = results.get('rerank_scores', [])
            
            if not rerank_ids:
                logger.warning("âš ï¸ Reranker æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£ã€‚")
                return {"final_docs": docs_to_rerank}
            
            reranked_docs = [docs_to_rerank[i] for i in rerank_ids]

            # --- æ‰“å°æ’åºç»“æœ ---
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ¯ Reranker æ‰“åˆ†ç»“æœ (ç½®ä¿¡åº¦ä»é«˜åˆ°ä½)")
            logger.info("=" * 60)
            for idx, (doc, score) in enumerate(zip(reranked_docs, rerank_scores), 1):
                # å°† rerank åˆ†æ•°æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­ï¼Œæ–¹ä¾¿è¿½è¸ª
                doc.metadata['rerank_score'] = score
                content_snippet = doc.page_content[:100].replace("\n", " ")
                if len(doc.page_content) > 100:
                    content_snippet += "..."
                logger.info(f"  [{idx}] åˆ†æ•°: {score:.4f} | æ¥æº: {doc.metadata.get('file_path', 'æœªçŸ¥')}")
                logger.info(f"      å†…å®¹: {content_snippet}")
            logger.info("=" * 60 + "\n")
            
            # ä» reranker é…ç½®ä¸­è·å– top_kï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 3
            top_k = getattr(reranker, 'rerank_top_k', 3)
            final_docs = reranked_docs[:top_k]
            
            logger.info(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å– Top {len(final_docs)} æ–‡æ¡£ä¼ é€’ç»™ç”ŸæˆèŠ‚ç‚¹ã€‚")
            
            # å°†ç²¾æ’åçš„æœ€ç»ˆæ–‡æ¡£åˆ—è¡¨æ”¾å…¥ state
            return {"final_docs": final_docs}
            
        except Exception as e:
            logger.error(f"âŒ ç²¾æ’è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            logger.warning("âš ï¸ ç²¾æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢æ–‡æ¡£ã€‚")
            return {"final_docs": docs_to_rerank}

    # --- æ­¥éª¤3: ä¿®æ”¹ generate_answer èŠ‚ç‚¹ï¼Œä½¿å…¶åªè´Ÿè´£ç”Ÿæˆ ---
    async def generate_answer(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: åŸºäºç²¾æ’åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        è¿™ä¸ªèŠ‚ç‚¹ä¸å†æ‰§è¡Œä»»ä½•æ£€ç´¢ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šgenerate_answer (ç”Ÿæˆç­”æ¡ˆ) ---")
        try:
            query = state["query"]
            # ä» state ä¸­è·å–ç”± rerank èŠ‚ç‚¹æä¾›çš„æœ€ç»ˆæ–‡æ¡£
            final_docs = state.get("final_docs", [])
            # è·å–å®ä½“å’Œå…³ç³»ä¿¡æ¯
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            
            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "context": {
                        "raw_context": "",
                        "query_mode": state.get("query_mode", "hybrid"),
                    }
                }

            # æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰
            kg_context_parts = []
            
            # æ·»åŠ å®ä½“ä¿¡æ¯
            if retrieved_entities:
                entity_context = "### ç›¸å…³å®ä½“\n\n"
                for idx, entity in enumerate(retrieved_entities[:10], 1):  # é™åˆ¶å‰10ä¸ª
                    entity_name = entity.get('entity_name', 'æœªçŸ¥')
                    entity_type = entity.get('entity_type', 'æœªçŸ¥')
                    description = entity.get('description', 'æ— æè¿°')
                    entity_context += f"{idx}. **{entity_name}** ({entity_type})\n   {description}\n\n"
                kg_context_parts.append(entity_context)
            
            # æ·»åŠ å…³ç³»ä¿¡æ¯
            if retrieved_relationships:
                relation_context = "### ç›¸å…³å…³ç³»\n\n"
                for idx, rel in enumerate(retrieved_relationships[:10], 1):  # é™åˆ¶å‰10æ¡
                    src = rel.get('src_id', '?')
                    tgt = rel.get('tgt_id', '?')
                    desc = rel.get('description', 'æ— æè¿°')
                    weight = rel.get('weight', 0)
                    relation_context += f"{idx}. {src} â†’ {tgt} (æƒé‡: {weight:.2f})\n   {desc}\n\n"
                kg_context_parts.append(relation_context)
            
            # å°†æœ€ç»ˆæ–‡æ¡£æ ¼å¼åŒ–ä¸ºé«˜è´¨é‡çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            doc_context_parts = []
            if final_docs:
                doc_context_parts.append("### ç›¸å…³æ–‡æ¡£\n")
                for idx, doc in enumerate(final_docs, 1):
                    rerank_score = doc.metadata.get('rerank_score', 'N/A')
                    score_str = f"{rerank_score:.4f}" if isinstance(rerank_score, float) else str(rerank_score)
                    chunk_id = doc.metadata.get('chunk_id', 'æœªçŸ¥')
                    file_path = doc.metadata.get('file_path', 'æœªçŸ¥')
                    
                    doc_context_parts.append(
                        f"ã€æ–‡æ¡£ {idx}ã€‘\n"
                        f"Chunk ID: {chunk_id}\n"
                        f"æ¥æº: {file_path}\n"
                        f"ç½®ä¿¡åº¦: {score_str}\n"
                        f"å†…å®¹:\n{doc.page_content}\n"
                    )
            
            # ç»„åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
            kg_context_str = "\n".join(kg_context_parts) if kg_context_parts else ""
            doc_context_str = "\n" + ("-" * 60 + "\n").join(doc_context_parts) if doc_context_parts else ""
            
            # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            full_context = ""
            if kg_context_str:
                full_context += "## çŸ¥è¯†å›¾è°±ä¿¡æ¯\n\n" + kg_context_str + "\n"
            if doc_context_str:
                full_context += "## æ–‡æ¡£å†…å®¹\n" + doc_context_str
            
            # æ„å»ºå‘é€ç»™ LLM çš„æç¤ºè¯
            system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±åŒ…å«äº†ä»æ–‡æ¡£ä¸­æå–çš„å®ä½“å’Œå…³ç³»ï¼Œæä¾›äº†ç»“æ„åŒ–çš„çŸ¥è¯†è§†å›¾ã€‚
æ–‡æ¡£å†…å®¹æ˜¯ç»è¿‡ç²¾æ’çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

å›ç­”æ—¶è¯·ï¼š
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ç†è§£å®ä½“é—´çš„å…³ç³»
2. ç»“åˆæ–‡æ¡£å†…å®¹æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ”¯æŒ
3. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„å®ä½“ã€å…³ç³»æˆ–æ–‡æ¡£æ¥æº
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥

--- ç›¸å…³ä¸Šä¸‹æ–‡ ---
{full_context}
--- ä¸Šä¸‹æ–‡ç»“æŸ ---
'''
            
            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")
            
            # ä½¿ç”¨ 'bypass' æ¨¡å¼è°ƒç”¨ aquery_llmï¼Œè¿™ä¼šè·³è¿‡ LightRAG å†…éƒ¨çš„æ£€ç´¢
            # ç›´æ¥å°†æˆ‘ä»¬çš„ system_prompt å’Œ query å‘é€ç»™ LLM
            result = await self.rag.aquery_llm(
                query,
                param=QueryParam(mode="bypass"),  # å…³é”®: è·³è¿‡å†…éƒ¨æ£€ç´¢
                system_prompt=system_prompt       # å…³é”®: æ³¨å…¥æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡
            )
            
            # ä»è¿”å›çš„å¤æ‚å­—å…¸ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
            answer = result.get("llm_response", {}).get("content", "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå›å¤ã€‚")
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
            
            return {
                "answer": answer,
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "answer": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "context": {}
            }