from typing import Dict, Any, TypedDict, Literal, Optional, List
#æ–°å¢Documentå¯¼å…¥
from langchain_core.documents import Document
from .light_graph_rag import LightRAG
from .state import IndexingState, QueryState
from .utils import logger
from .base import QueryParam

class WorkflowNodes:
    def __init__(self, rag_instance: LightRAG):
        self.rag = rag_instance

    # === Indexing: å•ä¸€èŠ‚ç‚¹è°ƒç”¨ ainsert ===
    async def index_documents(self, state: IndexingState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:è§¦å‘ LightRAG çš„å®Œæ•´ç´¢å¼•æµç¨‹ã€‚
        å°†å¤æ‚é˜Ÿåˆ—é€»è¾‘å°è£…åœ¨ ainsert å†…éƒ¨,LangGraph åªè´Ÿè´£å¯åŠ¨å’Œç›‘æ§ã€‚
        """
        logger.info("--- æ­£åœ¨è¿è¡ŒèŠ‚ç‚¹ï¼šindex_documents ---")
        try:
            track_id = await self.rag.ainsert(
                input=state["inputs"],
                ids=state.get("ids"),
                file_paths=state.get("file_paths")
            )
            logger.info(f"âœ… ç´¢å¼•ä»»åŠ¡å·²æäº¤,Track ID: {track_id}")
            return {
                "track_id": track_id,
                "status_message": "Document indexing started successfully."
            }
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return {
                "track_id": None,
                "status_message": f"Indexing failed: {str(e)}"
            }

    # === Querying ===
    async def retrieve_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:ä»çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šretrieve_context ---")
        try:
            query = state["query"]
            query_mode = state.get("query_mode", "hybrid")
            # # ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥ state ä¸­çš„å…³é”®ä¿¡æ¯
            # logger.info(f"ğŸ“¦ State ä¿¡æ¯:")
            # logger.info(f"   - query: {query}")
            # logger.info(f"   - query_mode: {query_mode}")
            # logger.info(f"   - reranker å­˜åœ¨: {'reranker' in state}")
            # logger.info(f"   - reranker å€¼: {state.get('reranker')}")
            
            # è°ƒç”¨ LightRAG çš„ aquery_data æ–¹æ³•ï¼Œå®ƒåªæ£€ç´¢æ•°æ®è€Œä¸è°ƒç”¨ LLM
            # æˆ‘ä»¬æ£€ç´¢æ›´å¤šçš„æ–‡æ¡£ï¼ˆä¾‹å¦‚ 20 ä¸ªï¼‰ä»¥ä¾›ç²¾æ’
            logger.info(f"æ­£åœ¨ä»¥ '{query_mode}' æ¨¡å¼ä¸ºæŸ¥è¯¢è¿›è¡Œç²—æ’æ£€ç´¢...")
            retrieval_result = await self.rag.aquery_data(
                query,
                param=QueryParam(mode=query_mode, chunk_top_k=20)
            )
            
            # ä»è¿”å›çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–æ‰€æœ‰ä¿¡æ¯
            data = retrieval_result.get("data", {})
            retrieved_chunks_data = data.get("chunks", [])
            retrieved_entities = data.get("entities", [])
            retrieved_relationships = data.get("relationships", [])
            
            # å°†å­—å…¸æ ¼å¼çš„ chunks è½¬æ¢ä¸º LangChain çš„ Document å¯¹è±¡ï¼Œä»¥ä¾¿åç»­å¤„ç†
            retrieved_docs = [
                Document(
                    page_content=chunk.get("content", ""),
                    metadata={
                        "file_path": chunk.get("file_path"),
                        "chunk_id": chunk.get("chunk_id"),
                        "reference_id": chunk.get("reference_id"),
                    }
                ) for chunk in retrieved_chunks_data
            ]
            
            logger.info(f"âœ… ç²—æ’æ£€ç´¢å®Œæˆ:")
            logger.info(f"   - æ–‡æ¡£å—: {len(retrieved_docs)} ä¸ª")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            
            # # æ‰“å°å®ä½“ä¿¡æ¯
            # if retrieved_entities:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ“Š æ£€ç´¢åˆ°çš„å®ä½“")
            #     logger.info("=" * 60)
            #     for idx, entity in enumerate(retrieved_entities[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            #         logger.info(f"  [{idx}] {entity.get('entity_name', 'æœªçŸ¥')}")
            #         logger.info(f"      ç±»å‹: {entity.get('entity_type', 'æœªçŸ¥')}")
            #         logger.info(f"      æè¿°: {entity.get('description', 'æ— ')[:100]}")
            #     if len(retrieved_entities) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_entities) - 5} ä¸ªå®ä½“")
            #     logger.info("=" * 60 + "\n")
            
            # # æ‰“å°å…³ç³»ä¿¡æ¯
            # if retrieved_relationships:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ”— æ£€ç´¢åˆ°çš„å…³ç³»")
            #     logger.info("=" * 60)
            #     for idx, rel in enumerate(retrieved_relationships[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
            #         logger.info(f"  [{idx}] {rel.get('src_id', '?')} â†’ {rel.get('tgt_id', '?')}")
            #         logger.info(f"      å…³ç³»: {rel.get('description', 'æ— ')[:100]}")
            #         logger.info(f"      æƒé‡: {rel.get('weight', 0):.2f}")
            #     if len(retrieved_relationships) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_relationships) - 5} æ¡å…³ç³»")
            #     logger.info("=" * 60 + "\n")
            
            # å°†åŸå§‹æ–‡æ¡£åˆ—è¡¨å’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯æ”¾å…¥ stateï¼Œä¼ é€’ç»™ rerank èŠ‚ç‚¹
            return {
                "retrieved_docs": retrieved_docs,
                "retrieved_entities": retrieved_entities,
                "retrieved_relationships": retrieved_relationships
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {e}")
            return {
                "retrieved_docs": [],
                "retrieved_entities": [],
                "retrieved_relationships": []
            }
        
    async def rerank_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: ä½¿ç”¨ BCE Reranker å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œç²¾æ’ï¼Œå¹¶æ‰“å°åˆ†æ•°ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šrerank_context (ç²¾æ’) ---")

        # # ğŸ”§ è¯¦ç»†è°ƒè¯•æ—¥å¿—
        # logger.info(f"ğŸ“¦ ç²¾æ’èŠ‚ç‚¹æ¥æ”¶åˆ°çš„ State ä¿¡æ¯:")
        # logger.info(f"   - State keys: {list(state.keys())}")
        # logger.info(f"   - 'reranker' in state: {'reranker' in state}")
        # logger.info(f"   - state.get('reranker'): {state.get('reranker')}")
        # logger.info(f"   - type(state.get('reranker')): {type(state.get('reranker'))}")
        # logger.info(f"   - 'retrieved_docs' in state: {'retrieved_docs' in state}")
        # logger.info(f"   - retrieved_docs æ•°é‡: {len(state.get('retrieved_docs', []))}")

        reranker = state.get("reranker")
        docs_to_rerank = state.get("retrieved_docs", [])

        # ğŸ”§ å¢å¼ºåˆ¤æ–­é€»è¾‘
        if reranker is None:
            logger.warning("âš ï¸ Reranker æœªé…ç½® (state['reranker'] is None)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": docs_to_rerank}  # ç›´æ¥å°†åŸå§‹æ–‡æ¡£ä¼ é€’ä¸‹å»
        
        if not docs_to_rerank:
            logger.warning("âš ï¸ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ (retrieved_docs ä¸ºç©º)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": []}

        try:
            query = state["query"]
            passages = [doc.page_content for doc in docs_to_rerank]
            
            logger.info(f"ğŸ¯ å¼€å§‹ç²¾æ’: å¯¹ {len(passages)} ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ–°æ’åº...")
            
            # è°ƒç”¨ reranker æ¨¡å‹è¿›è¡Œè®¡ç®—
            results = reranker.rerank(query, passages)
            rerank_ids = results.get('rerank_ids', [])
            rerank_scores = results.get('rerank_scores', [])
            
            if not rerank_ids:
                logger.warning("âš ï¸ Reranker æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£ã€‚")
                return {"final_docs": docs_to_rerank}
            
            reranked_docs = [docs_to_rerank[i] for i in rerank_ids]

            # --- æ‰“å°æ’åºç»“æœ ---
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ¯ Reranker æ‰“åˆ†ç»“æœ (ç½®ä¿¡åº¦ä»é«˜åˆ°ä½)")
            logger.info("=" * 60)
            for idx, (doc, score) in enumerate(zip(reranked_docs, rerank_scores), 1):
                # å°† rerank åˆ†æ•°æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­ï¼Œæ–¹ä¾¿è¿½è¸ª
                doc.metadata['rerank_score'] = score
                content_snippet = doc.page_content[:100].replace("\n", " ")
                if len(doc.page_content) > 100:
                    content_snippet += "..."
                logger.info(f"  [{idx}] åˆ†æ•°: {score:.4f} | æ¥æº: {doc.metadata.get('file_path', 'æœªçŸ¥')}")
                logger.info(f"      å†…å®¹: {content_snippet}")
            logger.info("=" * 60 + "\n")
            
            # ä» reranker é…ç½®ä¸­è·å– top_kï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 3
            top_k = getattr(reranker, 'rerank_top_k', 3)
            final_docs = reranked_docs[:top_k]
            
            logger.info(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å– Top {len(final_docs)} æ–‡æ¡£ä¼ é€’ç»™ç”ŸæˆèŠ‚ç‚¹ã€‚")
            
            # å°†ç²¾æ’åçš„æœ€ç»ˆæ–‡æ¡£åˆ—è¡¨æ”¾å…¥ state
            return {"final_docs": final_docs}
            
        except Exception as e:
            logger.error(f"âŒ ç²¾æ’è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            logger.warning("âš ï¸ ç²¾æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢æ–‡æ¡£ã€‚")
            return {"final_docs": docs_to_rerank}

    # --- æ­¥éª¤3: ä¿®æ”¹ generate_answer èŠ‚ç‚¹ï¼Œä½¿å…¶åªè´Ÿè´£ç”Ÿæˆ ---
    async def generate_answer(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: åŸºäºç²¾æ’åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        è¿™ä¸ªèŠ‚ç‚¹ä¸å†æ‰§è¡Œä»»ä½•æ£€ç´¢ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šgenerate_answer (ç”Ÿæˆç­”æ¡ˆ) ---")
        try:
            query = state["query"]
            # ä» state ä¸­è·å–ç”± rerank èŠ‚ç‚¹æä¾›çš„æœ€ç»ˆæ–‡æ¡£
            final_docs = state.get("final_docs", [])
            # è·å–å®ä½“å’Œå…³ç³»ä¿¡æ¯
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            
            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "context": {
                        "raw_context": "",
                        "query_mode": state.get("query_mode", "hybrid"),
                    }
                }
            
            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡
            full_context = self._build_context(
                final_docs, retrieved_entities, retrieved_relationships
            )
            
            # æ„å»ºå‘é€ç»™ LLM çš„æç¤ºè¯
            system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±åŒ…å«äº†ä»æ–‡æ¡£ä¸­æå–çš„å®ä½“å’Œå…³ç³»ï¼Œæä¾›äº†ç»“æ„åŒ–çš„çŸ¥è¯†è§†å›¾ã€‚
æ–‡æ¡£å†…å®¹æ˜¯ç»è¿‡ç²¾æ’çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

å›ç­”æ—¶è¯·ï¼š
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ç†è§£å®ä½“é—´çš„å…³ç³»
2. ç»“åˆæ–‡æ¡£å†…å®¹æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ”¯æŒ
3. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„å®ä½“ã€å…³ç³»æˆ–æ–‡æ¡£æ¥æº
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥

--- ç›¸å…³ä¸Šä¸‹æ–‡ ---
{full_context}
--- ä¸Šä¸‹æ–‡ç»“æŸ ---
'''
            
            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")
            
            # ä½¿ç”¨ 'bypass' æ¨¡å¼è°ƒç”¨ aquery_llmï¼Œè¿™ä¼šè·³è¿‡ LightRAG å†…éƒ¨çš„æ£€ç´¢
            # ç›´æ¥å°†æˆ‘ä»¬çš„ system_prompt å’Œ query å‘é€ç»™ LLM
            result = await self.rag.aquery_llm(
                query,
                param=QueryParam(mode="bypass"),  # å…³é”®: è·³è¿‡å†…éƒ¨æ£€ç´¢
                system_prompt=system_prompt       # å…³é”®: æ³¨å…¥æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡
            )
            
            # ä»è¿”å›çš„å¤æ‚å­—å…¸ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
            answer = result.get("llm_response", {}).get("content", "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå›å¤ã€‚")
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
            
            return {
                "answer": answer,
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None,
                    "entities": retrieved_entities,
                    "relationships": retrieved_relationships,
                    "documents": [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        } for doc in final_docs
                    ]
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "answer": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "context": {}
            }
    
    async def generate_answer_stream(self, state: QueryState):
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆçš„å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
        å…ˆæµå¼è¾“å‡ºæ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åé€æ­¥yieldç­”æ¡ˆçš„æ¯ä¸ªtokenã€‚
        """
        logger.info("--- è¿è¡Œæµå¼ç”Ÿæˆï¼šgenerate_answer_stream ---")
        try:
            query = state["query"]
            final_docs = state.get("final_docs", [])
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            llm = state.get("llm")
            chat_history = state.get("chat_history", [])
            
            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                yield {
                    "type": "answer_chunk",
                    "content": "æŠ±æ­‰ï¼Œæ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "done": True
                }
                return

            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆä¸éæµå¼ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
            full_context = self._build_context(
                final_docs, retrieved_entities, retrieved_relationships
            )
            
            # å…ˆyieldä¸Šä¸‹æ–‡ä¿¡æ¯
            yield {
                "type": "context",
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None,
                    "entities": retrieved_entities,
                    "relationships": retrieved_relationships,
                    "documents": [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        } for doc in final_docs
                    ]
                }
            }
            
            # === ç¬¬ä¸€æ­¥ï¼šæ„å»ºå¹¶ç›´æ¥æ˜¾ç¤ºç³»ç»Ÿæ‰§è¡Œä¿¡æ¯ ===
            logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆæ€è€ƒæ¨ç†è¿‡ç¨‹...")
            
            # æ„å»ºè¯¦ç»†çš„å®ä½“å’Œå…³ç³»ä¿¡æ¯
            entities_info = "\n".join([
                f"  â€¢ {e.get('entity_name', 'æœªçŸ¥')} ({e.get('entity_type', 'æœªçŸ¥ç±»å‹')})"
                for e in retrieved_entities[:5]
            ]) if retrieved_entities else "  (æ— ç›¸å…³å®ä½“)"
            
            relationships_info = "\n".join([
                f"  â€¢ {r.get('src_id', '?')} â†’ {r.get('tgt_id', '?')}"
                for r in retrieved_relationships[:3]
            ]) if retrieved_relationships else "  (æ— ç›¸å…³å…³ç³»)"
            
            docs_info = "\n".join([
                f"  â€¢ æ–‡æ¡£ {i+1}: {doc.metadata.get('file_path', 'æœªçŸ¥æ¥æº').split('/')[-1]} (ç½®ä¿¡åº¦: {doc.metadata.get('rerank_score', 0):.2f})"
                for i, doc in enumerate(final_docs[:3])
            ]) if final_docs else "  (æ— ç›¸å…³æ–‡æ¡£)"
            
            # æ„å»ºç³»ç»Ÿä¿¡æ¯ï¼ˆè¿™éƒ¨åˆ†ç›´æ¥æ˜¾ç¤ºï¼Œä¸ä¾èµ–LLMï¼‰
            system_info = f"""ğŸ“Š **ç³»ç»Ÿæ£€ç´¢ä¿¡æ¯**

**æ£€ç´¢é˜¶æ®µï¼š**
â€¢ æ£€ç´¢åˆ° {len(retrieved_entities)} ä¸ªç›¸å…³å®ä½“
â€¢ æ£€ç´¢åˆ° {len(retrieved_relationships)} æ¡ç›¸å…³å…³ç³»
â€¢ åˆæ­¥æ£€ç´¢åˆ°å¤šä¸ªæ–‡æ¡£ç‰‡æ®µ

**ç²¾æ’é˜¶æ®µï¼š**
â€¢ ç²¾æ’åä¿ç•™ {len(final_docs)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£
â€¢ ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦é‡æ–°æ’åº

**å…³é”®å®ä½“ï¼ˆå‰5ä¸ªï¼‰ï¼š**
{entities_info}

**å…³é”®å…³ç³»ï¼ˆå‰3ä¸ªï¼‰ï¼š**
{relationships_info}

**ç²¾æ’æ–‡æ¡£ï¼ˆå‰3ä¸ªï¼‰ï¼š**
{docs_info}

---

ğŸ’­ **æ¨ç†åˆ†æï¼š**
"""
            
            # ç›´æ¥yieldç³»ç»Ÿä¿¡æ¯ï¼ˆä¿è¯100%æ˜¾ç¤ºï¼‰
            logger.info(f"ğŸ“‹ ç›´æ¥æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯ ({len(system_info)} å­—ç¬¦)")
            yield {
                "type": "reasoning_chunk",
                "content": system_info,
                "done": False
            }
            
            # === ç¬¬äºŒæ­¥ï¼šè®©LLMè¡¥å……æ¨ç†åˆ†æ ===
            reasoning_messages = []
            
            # ç®€åŒ–çš„ç³»ç»Ÿæç¤ºï¼ˆåªè¦æ±‚æ¨ç†åˆ†æï¼Œä¸è¦æ±‚é‡å¤ç³»ç»Ÿä¿¡æ¯ï¼‰
            reasoning_system_prompt = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
ç³»ç»Ÿå·²ç»å±•ç¤ºäº†æ£€ç´¢å’Œç²¾æ’çš„è¯¦ç»†ä¿¡æ¯ã€‚

ç°åœ¨è¯·ç®€è¦è¯´æ˜ä½ çš„æ¨ç†åˆ†æè¿‡ç¨‹ï¼š
1. ä½ å¦‚ä½•ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼ˆ1-2å¥è¯ï¼‰
2. ä»æ£€ç´¢ç»“æœä¸­å‘ç°çš„å…³é”®ä¿¡æ¯ï¼ˆ2-3å¥è¯ï¼‰
3. ä½ çš„æ¨ç†é€»è¾‘ï¼ˆ2-3å¥è¯ï¼‰

è¦æ±‚ï¼š
- ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘ç†è§£..."ã€"æˆ‘å‘ç°..."ï¼‰
- ç®€æ´æ˜äº†ï¼Œæ€»å­—æ•°100-200å­—
- ä¸è¦é‡å¤ç³»ç»Ÿå·²å±•ç¤ºçš„ä¿¡æ¯'''
            
            reasoning_messages.append({"role": "system", "content": reasoning_system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯
            if chat_history:
                for msg in chat_history[-4:]:
                    reasoning_messages.append({
                        "role": msg.get("role"),
                        "content": msg.get("content")
                    })
            
            # ç®€åŒ–çš„ç”¨æˆ·æ¶ˆæ¯
            reasoning_user_message = f"""ç”¨æˆ·é—®é¢˜: {query}

ç³»ç»Ÿå·²æ£€ç´¢åˆ°:
- {len(retrieved_entities)} ä¸ªå®ä½“
- {len(retrieved_relationships)} æ¡å…³ç³»
- {len(final_docs)} ä¸ªç²¾æ’æ–‡æ¡£

è¯·ç®€è¦è¯´æ˜ä½ çš„æ¨ç†åˆ†æï¼ˆ100-200å­—ï¼‰ã€‚"""
            
            reasoning_messages.append({"role": "user", "content": reasoning_user_message})
            
            # æµå¼ç”ŸæˆLLMæ¨ç†éƒ¨åˆ†
            llm_reasoning = ""
            chunk_count = 0
            async for chunk in llm.astream(reasoning_messages):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                llm_reasoning += content
                chunk_count += 1
                
                # æ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡è¿›åº¦
                if chunk_count % 10 == 0:
                    logger.info(f"ğŸ’­ LLMæ¨ç†è¿›åº¦: å·²ç”Ÿæˆ {len(llm_reasoning)} å­—ç¬¦ ({chunk_count} chunks)")
                
                yield {
                    "type": "reasoning_chunk",
                    "content": content,
                    "done": False
                }
            
            # æ€è€ƒè¿‡ç¨‹å®Œæˆ
            full_reasoning = system_info + llm_reasoning
            logger.info(f"âœ… æ€è€ƒæ¨ç†å®Œæˆ: ç³»ç»Ÿä¿¡æ¯ {len(system_info)} å­—ç¬¦ + LLMæ¨ç† {len(llm_reasoning)} å­—ç¬¦ = æ€»è®¡ {len(full_reasoning)} å­—ç¬¦")
            yield {
                "type": "reasoning_chunk",
                "content": "",
                "done": True,
                "full_reasoning": full_reasoning
            }
            
            # === ç¬¬äºŒæ­¥ï¼šåŸºäºæ€è€ƒè¿‡ç¨‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ===
            logger.info("ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")
            
            answer_messages = []
            
            # ç­”æ¡ˆç”Ÿæˆçš„ç³»ç»Ÿæç¤º
            answer_system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±åŒ…å«äº†ä»æ–‡æ¡£ä¸­æå–çš„å®ä½“å’Œå…³ç³»ï¼Œæä¾›äº†ç»“æ„åŒ–çš„çŸ¥è¯†è§†å›¾ã€‚
æ–‡æ¡£å†…å®¹æ˜¯ç»è¿‡ç²¾æ’çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

å›ç­”æ—¶è¯·ï¼š
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ç†è§£å®ä½“é—´çš„å…³ç³»
2. ç»“åˆæ–‡æ¡£å†…å®¹æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ”¯æŒ
3. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„å®ä½“ã€å…³ç³»æˆ–æ–‡æ¡£æ¥æº
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥'''
            
            answer_messages.append({"role": "system", "content": answer_system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
            if chat_history:
                for msg in chat_history[-10:]:
                    answer_messages.append({
                        "role": msg.get("role"),
                        "content": msg.get("content")
                    })
            
            # æ·»åŠ åˆšæ‰çš„æ€è€ƒè¿‡ç¨‹ä½œä¸ºä¸Šä¸‹æ–‡
            answer_messages.append({
                "role": "assistant",
                "content": f"ã€æˆ‘çš„åˆ†ææ€è·¯ã€‘\n{full_reasoning}"
            })
            
            # æ·»åŠ å½“å‰æŸ¥è¯¢ï¼ˆåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
            answer_user_message = f"""è¯·åŸºäºä»¥ä¸‹ä¿é™©çŸ¥è¯†åº“ä¿¡æ¯å’Œä½ çš„åˆ†ææ€è·¯ï¼Œç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆ:

{full_context}

**ç”¨æˆ·é—®é¢˜:** {query}"""
            
            answer_messages.append({"role": "user", "content": answer_user_message})
            
            # æµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            full_answer = ""
            async for chunk in llm.astream(answer_messages):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += content
                yield {
                    "type": "answer_chunk",
                    "content": content,
                    "done": False
                }
            
            # æ ‡è®°å®Œæˆ
            logger.info(f"âœ… æµå¼ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(full_answer)} å­—ç¬¦)")
            yield {
                "type": "answer_chunk",
                "content": "",
                "done": True,
                "full_answer": full_answer,
                "full_reasoning": full_reasoning
            }
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield {
                "type": "answer_chunk",
                "content": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "done": True
            }
    
    def _build_context(self, final_docs, retrieved_entities, retrieved_relationships):
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆä¾›æµå¼å’Œéæµå¼ç‰ˆæœ¬å…±ç”¨ï¼‰"""
        # æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰
        kg_context_parts = []
        
        # æ·»åŠ å®ä½“ä¿¡æ¯
        if retrieved_entities:
            entity_context = "### ç›¸å…³å®ä½“\n\n"
            for idx, entity in enumerate(retrieved_entities[:10], 1):  # é™åˆ¶å‰10ä¸ª
                entity_name = entity.get('entity_name', 'æœªçŸ¥')
                entity_type = entity.get('entity_type', 'æœªçŸ¥')
                description = entity.get('description', 'æ— æè¿°')
                entity_context += f"{idx}. **{entity_name}** ({entity_type})\n   {description}\n\n"
            kg_context_parts.append(entity_context)
        
        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if retrieved_relationships:
            relation_context = "### ç›¸å…³å…³ç³»\n\n"
            for idx, rel in enumerate(retrieved_relationships[:10], 1):  # é™åˆ¶å‰10æ¡
                src = rel.get('src_id', '?')
                tgt = rel.get('tgt_id', '?')
                desc = rel.get('description', 'æ— æè¿°')
                weight = rel.get('weight', 0)
                relation_context += f"{idx}. {src} â†’ {tgt} (æƒé‡: {weight:.2f})\n   {desc}\n\n"
            kg_context_parts.append(relation_context)
        
        # å°†æœ€ç»ˆæ–‡æ¡£æ ¼å¼åŒ–ä¸ºé«˜è´¨é‡çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        doc_context_parts = []
        if final_docs:
            doc_context_parts.append("### ç›¸å…³æ–‡æ¡£\n")
            for idx, doc in enumerate(final_docs, 1):
                rerank_score = doc.metadata.get('rerank_score', 'N/A')
                score_str = f"{rerank_score:.4f}" if isinstance(rerank_score, float) else str(rerank_score)
                chunk_id = doc.metadata.get('chunk_id', 'æœªçŸ¥')
                file_path = doc.metadata.get('file_path', 'æœªçŸ¥')
                
                doc_context_parts.append(
                    f"ã€æ–‡æ¡£ {idx}ã€‘\n"
                    f"Chunk ID: {chunk_id}\n"
                    f"æ¥æº: {file_path}\n"
                    f"ç½®ä¿¡åº¦: {score_str}\n"
                    f"å†…å®¹:\n{doc.page_content}\n"
                )
        
        # ç»„åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
        kg_context_str = "\n".join(kg_context_parts) if kg_context_parts else ""
        doc_context_str = "\n" + ("-" * 60 + "\n").join(doc_context_parts) if doc_context_parts else ""
        
        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        full_context = ""
        if kg_context_str:
            full_context += "## çŸ¥è¯†å›¾è°±ä¿¡æ¯\n\n" + kg_context_str + "\n"
        if doc_context_str:
            full_context += "## æ–‡æ¡£å†…å®¹\n" + doc_context_str
        
        return full_context