from typing import Dict, Any, TypedDict, Literal, Optional, List
from pathlib import Path
from langchain_core.documents import Document
from .light_graph_rag import LightRAG
from .state import IndexingState, QueryState
from .utils import logger
from .base import QueryParam

class WorkflowNodes:
    def __init__(self, rag_instance: LightRAG):
        self.rag = rag_instance

    # === Indexing: å•ä¸€èŠ‚ç‚¹è°ƒç”¨ ainsert ===
    async def index_documents(self, state: IndexingState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:è§¦å‘ LightRAG çš„å®Œæ•´ç´¢å¼•æµç¨‹ã€‚
        å°†å¤æ‚é˜Ÿåˆ—é€»è¾‘å°è£…åœ¨ ainsert å†…éƒ¨,LangGraph åªè´Ÿè´£å¯åŠ¨å’Œç›‘æ§ã€‚
        """
        logger.info("--- æ­£åœ¨è¿è¡ŒèŠ‚ç‚¹ï¼šindex_documents ---")
        try:
            track_id = await self.rag.ainsert(
                input=state["inputs"],
                ids=state.get("ids"),
                file_paths=state.get("file_paths")
            )
            logger.info(f"âœ… ç´¢å¼•ä»»åŠ¡å·²æäº¤,Track ID: {track_id}")
            return {
                "track_id": track_id,
                "status_message": "Document indexing started successfully."
            }
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return {
                "track_id": None,
                "status_message": f"Indexing failed: {str(e)}"
            }

    # === Querying ===
    async def retrieve_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:ä»çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šretrieve_context ---")
        try:
            query = state["query"]
            query_mode = state.get("query_mode", "mix")
            
            logger.info(f"æ­£åœ¨ä»¥ '{query_mode}' æ¨¡å¼ä¸ºæŸ¥è¯¢è¿›è¡Œç²—æ’æ£€ç´¢...")
            retrieval_result = await self.rag.aquery_data(
                query,
                param=QueryParam(mode=query_mode, chunk_top_k=40)
            )
            
            # ä»è¿”å›çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–æ‰€æœ‰ä¿¡æ¯
            data = retrieval_result.get("data", {})
            retrieved_chunks_data = data.get("chunks", [])
            retrieved_entities = data.get("entities", [])
            retrieved_relationships = data.get("relationships", [])
            
            # å°†å­—å…¸æ ¼å¼çš„ chunks è½¬æ¢ä¸º LangChain çš„ Document å¯¹è±¡ï¼Œä»¥ä¾¿åç»­å¤„ç†
            retrieved_docs = [
                Document(
                    page_content=chunk.get("content", ""),
                    metadata={
                        "file_path": chunk.get("file_path"),
                        "chunk_id": chunk.get("chunk_id"),
                        "reference_id": chunk.get("reference_id"),
                    }
                ) for chunk in retrieved_chunks_data
            ]
            
            logger.info(f"âœ… ç²—æ’æ£€ç´¢å®Œæˆ:")
            logger.info(f"   - æ–‡æ¡£å—: {len(retrieved_docs)} ä¸ª")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            
            # # æ‰“å°å®ä½“ä¿¡æ¯
            # if retrieved_entities:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ“Š æ£€ç´¢åˆ°çš„å®ä½“")
            #     logger.info("=" * 60)
            #     for idx, entity in enumerate(retrieved_entities[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            #         logger.info(f"  [{idx}] {entity.get('entity_name', 'æœªçŸ¥')}")
            #         logger.info(f"      ç±»å‹: {entity.get('entity_type', 'æœªçŸ¥')}")
            #         logger.info(f"      æè¿°: {entity.get('description', 'æ— ')[:100]}")
            #     if len(retrieved_entities) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_entities) - 5} ä¸ªå®ä½“")
            #     logger.info("=" * 60 + "\n")
            
            # # æ‰“å°å…³ç³»ä¿¡æ¯
            # if retrieved_relationships:
            #     logger.info("\n" + "=" * 60)
            #     logger.info("ğŸ”— æ£€ç´¢åˆ°çš„å…³ç³»")
            #     logger.info("=" * 60)
            #     for idx, rel in enumerate(retrieved_relationships[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
            #         logger.info(f"  [{idx}] {rel.get('src_id', '?')} â†’ {rel.get('tgt_id', '?')}")
            #         logger.info(f"      å…³ç³»: {rel.get('description', 'æ— ')[:100]}")
            #         logger.info(f"      æƒé‡: {rel.get('weight', 0):.2f}")
            #     if len(retrieved_relationships) > 5:
            #         logger.info(f"  ... åŠå…¶ä»– {len(retrieved_relationships) - 5} æ¡å…³ç³»")
            #     logger.info("=" * 60 + "\n")
            
            # å°†åŸå§‹æ–‡æ¡£åˆ—è¡¨å’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯æ”¾å…¥ stateï¼Œä¼ é€’ç»™ rerank èŠ‚ç‚¹
            return {
                "retrieved_docs": retrieved_docs,
                "retrieved_entities": retrieved_entities,
                "retrieved_relationships": retrieved_relationships
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {e}")
            return {
                "retrieved_docs": [],
                "retrieved_entities": [],
                "retrieved_relationships": []
            }
        
    async def rerank_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: ä½¿ç”¨ BCE Reranker å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œç²¾æ’ï¼Œå¹¶æ‰“å°åˆ†æ•°ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šrerank_context (ç²¾æ’) ---")

        # # ğŸ”§ è¯¦ç»†è°ƒè¯•æ—¥å¿—
        # logger.info(f"ğŸ“¦ ç²¾æ’èŠ‚ç‚¹æ¥æ”¶åˆ°çš„ State ä¿¡æ¯:")
        # logger.info(f"   - State keys: {list(state.keys())}")
        # logger.info(f"   - 'reranker' in state: {'reranker' in state}")
        # logger.info(f"   - state.get('reranker'): {state.get('reranker')}")
        # logger.info(f"   - type(state.get('reranker')): {type(state.get('reranker'))}")
        # logger.info(f"   - 'retrieved_docs' in state: {'retrieved_docs' in state}")
        # logger.info(f"   - retrieved_docs æ•°é‡: {len(state.get('retrieved_docs', []))}")

        reranker = state.get("reranker")
        docs_to_rerank = state.get("retrieved_docs", [])

        # ğŸ”§ å¢å¼ºåˆ¤æ–­é€»è¾‘
        if reranker is None:
            logger.warning("âš ï¸ Reranker æœªé…ç½® (state['reranker'] is None)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": docs_to_rerank}  # ç›´æ¥å°†åŸå§‹æ–‡æ¡£ä¼ é€’ä¸‹å»
        
        if not docs_to_rerank:
            logger.warning("âš ï¸ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ (retrieved_docs ä¸ºç©º)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": []}

        try:
            query = state["query"]
            passages = [doc.page_content for doc in docs_to_rerank]
            
            logger.info(f"ğŸ¯ å¼€å§‹ç²¾æ’: å¯¹ {len(passages)} ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ–°æ’åº...")
            
            # è°ƒç”¨ reranker æ¨¡å‹è¿›è¡Œè®¡ç®—
            results = reranker.rerank(query, passages)
            rerank_ids = results.get('rerank_ids', [])
            rerank_scores = results.get('rerank_scores', [])
            
            if not rerank_ids:
                logger.warning("âš ï¸ Reranker æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£ã€‚")
                return {"final_docs": docs_to_rerank}
            
            reranked_docs = [docs_to_rerank[i] for i in rerank_ids]

            # --- æ‰“å°æ’åºç»“æœ ---
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ¯ Reranker æ‰“åˆ†ç»“æœ (ç½®ä¿¡åº¦ä»é«˜åˆ°ä½)")
            logger.info("=" * 60)
            for idx, (doc, score) in enumerate(zip(reranked_docs, rerank_scores), 1):
                # å°† rerank åˆ†æ•°æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­ï¼Œæ–¹ä¾¿è¿½è¸ª
                doc.metadata['rerank_score'] = score
                content_snippet = doc.page_content[:100].replace("\n", " ")
                if len(doc.page_content) > 100:
                    content_snippet += "..."
                logger.info(f"  [{idx}] åˆ†æ•°: {score:.4f} | æ¥æº: {doc.metadata.get('file_path', 'æœªçŸ¥')}")
                logger.info(f"      å†…å®¹: {content_snippet}")
            logger.info("=" * 60 + "\n")
            
            # ä» reranker é…ç½®ä¸­è·å– top_kï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 20
            top_k = getattr(reranker, 'rerank_top_k', 20)
            final_docs = reranked_docs[:top_k]
            
            logger.info(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å– Top {len(final_docs)} æ–‡æ¡£ä¼ é€’ç»™ç”ŸæˆèŠ‚ç‚¹ã€‚")
            
            # å°†ç²¾æ’åçš„æœ€ç»ˆæ–‡æ¡£åˆ—è¡¨æ”¾å…¥ state
            return {"final_docs": final_docs}
            
        except Exception as e:
            logger.error(f"âŒ ç²¾æ’è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            logger.warning("âš ï¸ ç²¾æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢æ–‡æ¡£ã€‚")
            return {"final_docs": docs_to_rerank}

    async def generate_answer(self, state: "QueryState") -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: åŸºäºç²¾æ’åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        ä½¿ç”¨ LangChain LLMï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹:generate_answer (ç”Ÿæˆç­”æ¡ˆ) ---")
        try:
            query = state["query"]
            final_docs = state.get("final_docs", [])
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            chat_history = state.get("chat_history", [])  # è·å–å¯¹è¯å†å²
            llm = state.get("llm")  # è·å– LLM å®ä¾‹
            
            if not llm:
                raise ValueError("âŒ LLM å®ä¾‹æœªåœ¨ state ä¸­é…ç½®")
            
            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                return {
                    "answer": "æŠ±æ­‰,æ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "chat_history": chat_history + [
                        {"role": "user", "content": query},
                        {"role": "assistant", "content": "æŠ±æ­‰,æ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
                    ],
                    "context": {
                        "raw_context": "",
                        "query_mode": state.get("query_mode", "hybrid"),
                    }
                }

            # ==================== æ ‡å‡†åŒ–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ ====================
            kg_context = self._format_knowledge_graph(retrieved_entities, retrieved_relationships)
            
            # ==================== æ ‡å‡†åŒ–æ–‡æ¡£ä¸Šä¸‹æ–‡ ====================
            doc_context = self._format_documents(final_docs)
            
            # ==================== æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ ====================
            full_context = ""
            if kg_context:
                full_context += "# ğŸ“Š çŸ¥è¯†å›¾è°±ä¿¡æ¯\n\n" + kg_context + "\n"
            if doc_context:
                full_context += "# ğŸ“„ ç›¸å…³æ¡æ¬¾æ–‡æ¡£\n\n" + doc_context
            
            # ==================== æ„å»ºä¿é™©é¢†åŸŸä¸“ç”¨æç¤ºè¯ ====================
            system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿é™©å’¨è¯¢é¡¾é—®ï¼Œæ“…é•¿è§£è¯»ä¿é™©æ¡æ¬¾ã€ç†èµ”è§„åˆ™å’Œäº§å“è¯´æ˜ã€‚

**ä½ çš„èŒè´£:**
1. åŸºäºçŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å…³ç³»ç†è§£ä¿é™©ä¸šåŠ¡é€»è¾‘
2. ç»“åˆæ–‡æ¡£åŸæ–‡æä¾›å‡†ç¡®çš„æ¡æ¬¾è§£é‡Š
3. ç”¨æ¸…æ™°æ˜“æ‡‚çš„è¯­è¨€è§£ç­”å®¢æˆ·ç–‘é—®

**å›ç­”åŸåˆ™:**
- **å‡†ç¡®æ€§ä¼˜å…ˆ**: ä¸¥æ ¼ä¾æ®æä¾›çš„ä¿é™©æ¡æ¬¾å’ŒçŸ¥è¯†å›¾è°±
- **ç»“æ„åŒ–è¡¨è¾¾**: ä½¿ç”¨åˆ†ç‚¹ã€åˆ†æ®µçš„æ–¹å¼ç»„ç»‡ç­”æ¡ˆ
- **å¼•ç”¨æ¥æº**: åœ¨å…³é”®ä¿¡æ¯åæ ‡æ³¨æ¥æºå®ä½“æˆ–æ¡æ¬¾
- **é£é™©æç¤º**: æ¶‰åŠå…è´£æ¡æ¬¾æ—¶éœ€ç‰¹åˆ«å¼ºè°ƒ
- **è¯šå®è¡¨è¾¾**: ä¿¡æ¯ä¸è¶³æ—¶æ˜ç¡®å‘ŠçŸ¥,ä¸å¯è‡†æµ‹

**å›ç­”æ ¼å¼å»ºè®®:**
1. ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜
2. åˆ—ä¸¾å…³é”®æ¡æ¬¾å’Œä¾æ®
3. è¡¥å……æ³¨æ„äº‹é¡¹æˆ–é™åˆ¶æ¡ä»¶
"""
            
            # ==================== æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰====================
            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨ LangChain LLM ç”Ÿæˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")
            logger.info(f"   - å¯¹è¯å†å²: {len(chat_history)} è½®")
            
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å¯¹è¯å†å²
            for turn in chat_history:
                messages.append({
                    "role": turn["role"],
                    "content": turn["content"]
                })
            
            # æ·»åŠ å½“å‰æŸ¥è¯¢ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            user_message = f"""è¯·åŸºäºä»¥ä¸‹ä¿é™©çŸ¥è¯†åº“ä¿¡æ¯å›ç­”é—®é¢˜:

{full_context}

**ç”¨æˆ·é—®é¢˜:** {query}"""
            
            messages.append({"role": "user", "content": user_message})
            
            # è°ƒç”¨ LangChain LLM
            try:
                response = await llm.ainvoke(messages)
                answer = response.content
            except Exception as e:
                logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                answer = f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"
            
            # ğŸ†• æ›´æ–°å¯¹è¯å†å²
            new_history = chat_history + [
                {"role": "user", "content": query},
                {"role": "assistant", "content": answer}
            ]
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
            
            return {
                "answer": answer,
                "chat_history": new_history,  # è¿”å›æ›´æ–°åçš„å¯¹è¯å†å²
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None,
                    "entities": retrieved_entities,  # ä¼ é€’ç»™å‰ç«¯å¯è§†åŒ–
                    "relationships": retrieved_relationships,  # ä¼ é€’ç»™å‰ç«¯å¯è§†åŒ–
                    "documents": [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        } for doc in final_docs
                    ]
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "answer": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "chat_history": chat_history,
                "context": {}
            }

    def _format_knowledge_graph(self, entities: List[Dict], relationships: List[Dict]) -> str:
        """æ ‡å‡†åŒ–æ ¼å¼åŒ–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡"""
        if not entities and not relationships:
            return ""
        
        kg_parts = []
        
        # æ ¼å¼åŒ–å®ä½“
        if entities:
            kg_parts.append("## ğŸ·ï¸ ç›¸å…³å®ä½“\n")
            for idx, entity in enumerate(entities[:10], 1):  # é™åˆ¶å‰10ä¸ª
                name = entity.get('entity_name', 'æœªçŸ¥')
                type_ = entity.get('entity_type', 'æœªçŸ¥ç±»å‹')
                desc = entity.get('description', 'æ— æè¿°')
                
                kg_parts.append(f"**[{idx}] {name}** `{type_}`\n")
                kg_parts.append(f"  â””â”€ {desc}\n\n")
        
        # æ ¼å¼åŒ–å…³ç³»
        if relationships:
            kg_parts.append("## ğŸ”— å®ä½“å…³ç³»\n")
            for idx, rel in enumerate(relationships[:10], 1):  # é™åˆ¶å‰10æ¡
                src = rel.get('src_id', '?')
                tgt = rel.get('tgt_id', '?')
                desc = rel.get('description', 'æ— æè¿°')
                weight = rel.get('weight', 0)
                
                kg_parts.append(f"**[{idx}]** {src} âœ {tgt} `æƒé‡:{weight:.2f}`\n")
                kg_parts.append(f"  â””â”€ {desc}\n\n")
        
        return "".join(kg_parts)

    def _format_documents(self, documents: List[Document]) -> str:
        """æ ‡å‡†åŒ–æ ¼å¼åŒ–æ–‡æ¡£ä¸Šä¸‹æ–‡"""
        if not documents:
            return ""
        
        doc_parts = []
        
        for idx, doc in enumerate(documents, 1):
            rerank_score = doc.metadata.get('rerank_score', 'N/A')
            score_str = f"{rerank_score:.4f}" if isinstance(rerank_score, float) else str(rerank_score)
            chunk_id = doc.metadata.get('chunk_id', 'æœªçŸ¥')
            file_path = doc.metadata.get('file_path', 'æœªçŸ¥æ¥æº')
            
            doc_parts.append(f"### ğŸ“‘ æ–‡æ¡£ç‰‡æ®µ {idx}\n")
            doc_parts.append(f"- **æ¥æºæ–‡ä»¶:** {Path(file_path).name}\n")
            doc_parts.append(f"- **ç‰‡æ®µID:** {chunk_id}\n")
            doc_parts.append(f"- **ç›¸å…³åº¦è¯„åˆ†:** {score_str}\n")
            doc_parts.append(f"\n**å†…å®¹:**\n```\n{doc.page_content}\n```\n\n")
            doc_parts.append("---\n\n")
        
        return "".join(doc_parts)