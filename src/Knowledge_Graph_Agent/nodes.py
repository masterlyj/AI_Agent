from typing import Dict, Any, TypedDict, Literal, Optional, List, Tuple, Set
from langchain_core.documents import Document
from .light_graph_rag import LightRAG
from .state import IndexingState, QueryState
from .utils import logger
from .base import QueryParam

class WorkflowNodes:
    def __init__(self, rag_instance: LightRAG):
        self.rag = rag_instance

    # === Indexing: å•ä¸€èŠ‚ç‚¹è°ƒç”¨ ainsert ===
    async def index_documents(self, state: IndexingState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:è§¦å‘ LightRAG çš„å®Œæ•´ç´¢å¼•æµç¨‹ã€‚
        å°†å¤æ‚é˜Ÿåˆ—é€»è¾‘å°è£…åœ¨ ainsert å†…éƒ¨,LangGraph åªè´Ÿè´£å¯åŠ¨å’Œç›‘æ§ã€‚
        """
        logger.info("--- æ­£åœ¨è¿è¡ŒèŠ‚ç‚¹ï¼šindex_documents ---")
        try:
            track_id = await self.rag.ainsert(
                input=state["inputs"],
                ids=state.get("ids"),
                file_paths=state.get("file_paths")
            )
            logger.info(f"âœ… å®ä½“ä¸å…³ç³»æŠ½å–ä»»åŠ¡å·²æäº¤,Track ID: {track_id}")
            return {
                "track_id": track_id,
                "status_message": "Document indexing started successfully."
            }
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return {
                "track_id": None,
                "status_message": f"Indexing failed: {str(e)}"
            }

    # === Querying ===
    async def retrieve_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹:ä»çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸Šä¸‹æ–‡
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šretrieve_context ---")
        try:
            query = state["query"]
            query_mode = state.get("query_mode", "hybrid")

            logger.info(f"æ­£åœ¨ä»¥ '{query_mode}' æ¨¡å¼ä¸ºæŸ¥è¯¢è¿›è¡Œç²—æ’æ£€ç´¢...")
            retrieval_result = await self.rag.aquery_data(
                query,
                param=QueryParam(mode=query_mode)
            )

            # ä»è¿”å›çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–æ‰€æœ‰ä¿¡æ¯
            data = retrieval_result.get("data", {})
            retrieved_chunks_data = data.get("chunks", [])
            retrieved_entities = data.get("entities", [])
            retrieved_relationships = data.get("relationships", [])

            # å°†å­—å…¸æ ¼å¼çš„ chunks è½¬æ¢ä¸º LangChain çš„ Document å¯¹è±¡ï¼Œä»¥ä¾¿åç»­å¤„ç†
            retrieved_docs = [
                Document(
                    page_content=chunk.get("content", ""),
                    metadata={
                        "file_path": chunk.get("file_path"),
                        "chunk_id": chunk.get("chunk_id"),
                        "reference_id": chunk.get("reference_id"),
                    }
                ) for chunk in retrieved_chunks_data
            ]

            logger.info(f"âœ… ç²—æ’æ£€ç´¢å®Œæˆ:")
            logger.info(f"   - æ–‡æ¡£å—: {len(retrieved_docs)} ä¸ª")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")

            return {
                "retrieved_docs": retrieved_docs,
                "retrieved_entities": retrieved_entities,
                "retrieved_relationships": retrieved_relationships
            }

        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {e}")
            return {
                "retrieved_docs": [],
                "retrieved_entities": [],
                "retrieved_relationships": []
            }
        
    async def rerank_context(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: ä½¿ç”¨ BCE Reranker å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œç²¾æ’ï¼Œå¹¶æ‰“å°åˆ†æ•°ã€‚
        ç²¾æ’åé€‰å– top_k æ–‡æ¡£ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šrerank_context (ç²¾æ’) ---")
        reranker = state.get("reranker")
        docs_to_rerank = state.get("retrieved_docs", [])

        # å¢å¼ºåˆ¤æ–­é€»è¾‘
        if reranker is None:
            logger.warning("âš ï¸ Reranker æœªé…ç½® (state['reranker'] is None)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": docs_to_rerank}  # ç›´æ¥å°†åŸå§‹æ–‡æ¡£ä¼ é€’ä¸‹å»

        if not docs_to_rerank:
            logger.warning("âš ï¸ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ (retrieved_docs ä¸ºç©º)ï¼Œè·³è¿‡ç²¾æ’æ­¥éª¤ã€‚")
            return {"final_docs": []}

        try:
            query = state["query"]
            passages = [doc.page_content for doc in docs_to_rerank]

            logger.info(f"ğŸ¯ å¼€å§‹ç²¾æ’: å¯¹ {len(passages)} ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ–°æ’åº...")

            # è°ƒç”¨ reranker æ¨¡å‹è¿›è¡Œè®¡ç®—
            results = reranker.rerank(query, passages)
            rerank_ids = results.get('rerank_ids', [])
            rerank_scores = results.get('rerank_scores', [])

            if not rerank_ids:
                logger.warning("âš ï¸ Reranker æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£ã€‚")
                return {"final_docs": docs_to_rerank}

            reranked_docs = [docs_to_rerank[i] for i in rerank_ids]

            for doc, score in zip(reranked_docs, rerank_scores):
                doc.metadata['rerank_score'] = score

            top_k = state.get("rerank_top_k") or getattr(reranker, 'rerank_top_k', 20)

            # ç›´æ¥é€‰å– top_k
            final_docs = reranked_docs[:top_k]

            logger.info(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å– Top {len(final_docs)} æ–‡æ¡£ä¼ é€’ç»™ç”ŸæˆèŠ‚ç‚¹ (ç”¨æˆ·è®¾ç½®: {state.get('rerank_top_k', 'æœªè®¾ç½®')})ã€‚")

            return {"final_docs": final_docs}

        except Exception as e:
            logger.error(f"âŒ ç²¾æ’è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            logger.warning("âš ï¸ ç²¾æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢æ–‡æ¡£ã€‚")
            return {"final_docs": docs_to_rerank}

    async def generate_answer(self, state: QueryState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹: åŸºäºç²¾æ’åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        è¿™ä¸ªèŠ‚ç‚¹ä¸å†æ‰§è¡Œä»»ä½•æ£€ç´¢ã€‚
        """
        logger.info("--- è¿è¡ŒèŠ‚ç‚¹ï¼šgenerate_answer (ç”Ÿæˆç­”æ¡ˆ) ---")
        try:
            query = state["query"]
            final_docs = state.get("final_docs", [])
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            chat_history = state.get("chat_history", [])
            llm = state.get("llm")
            if not llm:
                raise ValueError("âŒ LLM å®ä¾‹æœªåœ¨ state ä¸­é…ç½®")

            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                return {
                    "answer": "æŠ±æ­‰,æ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "chat_history": chat_history + [
                        {"role": "user", "content": query},
                        {"role": "assistant", "content": "æŠ±æ­‰,æ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
                    ],
                    "context": {
                        "raw_context": "",
                        "query_mode": state.get("query_mode", "hybrid"),
                    }
                }

            full_context = self._build_context(
                final_docs, retrieved_entities, retrieved_relationships
            )

            system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±åŒ…å«äº†ä»æ–‡æ¡£ä¸­æå–çš„å®ä½“å’Œå…³ç³»ï¼Œæä¾›äº†ç»“æ„åŒ–çš„çŸ¥è¯†è§†å›¾ã€‚
æ–‡æ¡£å†…å®¹æ˜¯ç»è¿‡ç²¾æ’çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

å›ç­”æ—¶è¯·ï¼š
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ç†è§£å®ä½“é—´çš„å…³ç³»
2. ç»“åˆæ–‡æ¡£å†…å®¹æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ”¯æŒ
3. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„å®ä½“ã€å…³ç³»æˆ–æ–‡æ¡£æ¥æº
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥

--- ç›¸å…³ä¸Šä¸‹æ–‡ ---
{full_context}
--- ä¸Šä¸‹æ–‡ç»“æŸ ---
'''

            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")

            result = await self.rag.aquery_llm(
                query,
                param=QueryParam(mode="bypass"),
                system_prompt=system_prompt
            )

            answer = result.get("llm_response", {}).get("content", "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆå›å¤ã€‚")

            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
            
            return {
                "answer": answer,
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None,
                    "entities": retrieved_entities,
                    "relationships": retrieved_relationships,
                    "documents": [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        } for doc in final_docs
                    ]
                }
            }

        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "answer": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "context": {}
            }
    
    async def generate_answer_stream(self, state: QueryState):
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆçš„å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
        å…ˆæµå¼è¾“å‡ºæ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åé€æ­¥yieldç­”æ¡ˆçš„æ¯ä¸ªtokenã€‚
        æ”¯æŒäºŒæ¬¡æ£€ç´¢ï¼ˆrequeryï¼‰ï¼šå¦‚æœLLMåˆ¤æ–­ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œä¼šè§¦å‘äºŒæ¬¡æ£€ç´¢åç»§ç»­ç”Ÿæˆç­”æ¡ˆã€‚
        
        ç‰¹æ®Šå¤„ç†ï¼š
        - å¦‚æœæ¨¡å‹æ˜¯ deepseek-reasonerï¼Œåˆ™è·³è¿‡æ·±åº¦æ€è€ƒé˜¶æ®µï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆ
        """
        try:
            query = state["query"]
            final_docs = state.get("final_docs", [])
            retrieved_entities = state.get("retrieved_entities", [])
            retrieved_relationships = state.get("retrieved_relationships", [])
            llm = state.get("llm")
            chat_history = state.get("chat_history", [])
            model_name = state.get("model_name", "").lower()  # è·å–æ¨¡å‹åç§°
            
            # åˆ¤æ–­æ˜¯å¦ä¸º deepseek-reasoner æ¨¡å‹
            skip_reasoning = "deepseek-reasoner" in model_name or "reasoner" in model_name
            
            if not final_docs and not retrieved_entities and not retrieved_relationships:
                logger.warning("âš ï¸ æ²¡æœ‰ä¸Šä¸‹æ–‡å¯ä¾›ç”Ÿæˆç­”æ¡ˆã€‚")
                yield {
                    "type": "answer_chunk",
                    "content": "æŠ±æ­‰ï¼Œæ ¹æ®å¯ç”¨ä¿¡æ¯æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "done": True
                }
                return

            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆæ·±åº¦æ€è€ƒé˜¶æ®µé™åˆ¶ä¸ºå‰20ä¸ªæœ€ç›¸å…³çš„é¡¹ç›®ï¼Œé¿å…ä¿¡æ¯è¿‡è½½ï¼‰
            full_context = self._build_context(
                final_docs, retrieved_entities, retrieved_relationships, max_items=20
            )
            
            # å…ˆyieldä¸Šä¸‹æ–‡ä¿¡æ¯
            yield {
                "type": "context",
                "context": {
                    "raw_context": full_context,
                    "query_mode": state.get("query_mode", "hybrid"),
                    "num_docs_used": len(final_docs),
                    "num_entities": len(retrieved_entities),
                    "num_relationships": len(retrieved_relationships),
                    "rerank_enabled": state.get("reranker") is not None,
                    "entities": retrieved_entities,
                    "relationships": retrieved_relationships,
                    "documents": [
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        } for doc in final_docs
                    ]
                }
            }
            
            # === æ·±åº¦æ€è€ƒï¼šè®©LLMçœ‹å®Œæ•´æ–‡æ¡£å†…å®¹åˆ¤æ–­ä¸Šä¸‹æ–‡æ˜¯å¦å……åˆ† ===
            # å¦‚æœæ˜¯ deepseek-reasoner æ¨¡å‹ï¼Œä½¿ç”¨å…¶å†…ç½®æ¨ç†èƒ½åŠ›ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
            if skip_reasoning:
                logger.info(f"âš¡ æ£€æµ‹åˆ° {state.get('model_name', 'unknown')} æ¨¡å‹ï¼Œä½¿ç”¨å†…ç½®æ¨ç†èƒ½åŠ›ç›´æ¥ç”Ÿæˆç­”æ¡ˆ...")
                
                # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                full_context = self._build_context(
                    final_docs, retrieved_entities, retrieved_relationships, max_items=20
                )
                
                # ç›´æ¥æ„å»ºç­”æ¡ˆç”Ÿæˆçš„æ¶ˆæ¯
                answer_system_prompt = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**âš ï¸ æå…¶é‡è¦ï¼šä¸åŒä¿é™©äº§å“çš„æ¡æ¬¾å·®å¼‚**
1. **äº§å“ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªä¿é™©äº§å“éƒ½æœ‰å…¶ç‹¬ç«‹çš„ä¿é™©æ¡æ¬¾ï¼Œä¸åŒäº§å“çš„æ¡æ¬¾å†…å®¹å¯èƒ½å®Œå…¨ä¸åŒã€‚
   - ä¾‹å¦‚ï¼šã€Œä¼ å®¶å®ã€ä¿é™©æ¡æ¬¾å¯èƒ½ä¸å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜
   - è€Œã€Œä¼ å®¶ç¦ã€ä¿é™©æ¡æ¬¾å¯èƒ½å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜
   
2. **ä¸¥æ ¼åŒ¹é…**ï¼šå›ç­”é—®é¢˜æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼åŒºåˆ†ä¸åŒçš„ä¿é™©äº§å“ï¼š
   - æ£€æŸ¥æ–‡æ¡£æ¥æºï¼ˆfile_pathã€chunk_idï¼‰ç¡®è®¤äº§å“åç§°
   - ä¸è¦å°†ä¸€ä¸ªäº§å“çš„æ¡æ¬¾å¥—ç”¨åˆ°å¦ä¸€ä¸ªäº§å“ä¸Š
   - å¦‚æœç”¨æˆ·è¯¢é—®çš„æ˜¯äº§å“Aï¼Œç»ä¸èƒ½ç”¨äº§å“Bçš„æ¡æ¬¾æ¥å›ç­”
   
3. **ä¿¡æ¯ä¸è¶³æ—¶çš„å¤„ç†**ï¼š
   - å¦‚æœå½“å‰ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·è¯¢é—®çš„å…·ä½“äº§å“çš„ç›¸å…³ä¿¡æ¯
   - æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"å½“å‰æ£€ç´¢çš„ä¿¡æ¯ä¸åŒ…å«è¯¥äº§å“çš„ç›¸å…³æ¡æ¬¾"
   - ä¸è¦çŒœæµ‹æˆ–ä½¿ç”¨å…¶ä»–äº§å“çš„ä¿¡æ¯è¿›è¡Œæ¨æ–­

**æ–‡æ¡£æ ¼å¼è¯´æ˜**
æ–‡æ¡£å†…å®¹å¯èƒ½åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œå…¶æ ¼å¼å¦‚ä¸‹ï¼š
- `[SOURCE:__TABLE_ENTITY_X__]` è¡¨ç¤ºè¯¥å—å¯¹åº”çš„è¡¨æ ¼å ä½ç¬¦ï¼ŒXä¸ºè¡¨æ ¼ç¼–å·
- `[CONTEXT]` åé¢æ˜¯ä¸è¯¥è¡¨æ ¼ç›¸å…³çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
- `[HTML_TABLE]` åé¢æ˜¯çœŸå®è¡¨æ ¼çš„HTMLå†…å®¹

**å›ç­”è¦æ±‚**
è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³æˆ–ä¸ç¡®å®šï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚'''

                answer_messages = [{"role": "system", "content": answer_system_prompt}]
                
                # æ·»åŠ å†å²å¯¹è¯
                if chat_history:
                    for msg in chat_history[-4:]:
                        answer_messages.append({
                            "role": msg.get("role"),
                            "content": msg.get("content")
                        })
                
                # æ·»åŠ å½“å‰é—®é¢˜å’Œä¸Šä¸‹æ–‡
                answer_user_message = f"""**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š**

{full_context}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
                
                answer_messages.append({"role": "user", "content": answer_user_message})
                
                # æµå¼ç”Ÿæˆç­”æ¡ˆï¼ŒDeepSeek Reasonerä¼šé€šè¿‡reasoning_contentå­—æ®µè¿”å›æ¨ç†è¿‡ç¨‹
                # æ³¨æ„ï¼šLangChainçš„astreamæ— æ³•æ­£ç¡®è·å–reasoning_contentï¼Œéœ€è¦ç›´æ¥ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
                import asyncio
                from openai import AsyncOpenAI
                import os
                
                # åˆ›å»ºOpenAIå…¼å®¹çš„å®¢æˆ·ç«¯ï¼ˆDeepSeek APIå…¼å®¹OpenAIæ ¼å¼ï¼‰
                client = AsyncOpenAI(
                    api_key=os.getenv("DEEPSEEK_API_KEY"),
                    base_url=os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1")
                )
                
                full_answer = ""
                reasoning_content = ""
                answer_content = ""
                reasoning_chunk_count = 0
                answer_chunk_count = 0
                reasoning_buffer = ""
                answer_buffer = ""
                reasoning_buffer_size = 0
                answer_buffer_size = 0
                reasoning_done = False
                
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯çš„æµå¼API
                stream = await client.chat.completions.create(
                    model=state.get("model_name", "deepseek-reasoner"),
                    messages=answer_messages,
                    stream=True
                )
                
                async for chunk in stream:
                    if not chunk.choices:
                        continue
                    
                    delta = chunk.choices[0].delta
                    
                    # æå–æ¨ç†å†…å®¹ï¼ˆDeepSeekç‰¹æœ‰å­—æ®µï¼‰
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_part = delta.reasoning_content
                        reasoning_content += reasoning_part
                        reasoning_buffer += reasoning_part
                        reasoning_buffer_size += len(reasoning_part)
                        reasoning_chunk_count += 1
                        
                        # æ¨ç†è¿‡ç¨‹ç¼“å†²è¾“å‡º
                        if reasoning_buffer_size >= 20 or reasoning_chunk_count % 5 == 0:
                            yield {
                                "type": "reasoning_chunk",
                                "content": reasoning_buffer,
                                "done": False
                            }
                            reasoning_buffer = ""
                            reasoning_buffer_size = 0
                            await asyncio.sleep(0.01)
                    
                    # æå–ç­”æ¡ˆå†…å®¹
                    if hasattr(delta, 'content') and delta.content:
                        # å¦‚æœæ¨ç†åˆšå®Œæˆä¸”è¿˜æ²¡æ ‡è®°ï¼Œå…ˆæ ‡è®°æ¨ç†å®Œæˆ
                        if reasoning_content and not reasoning_done:
                            # å‘é€å‰©ä½™æ¨ç†ç¼“å†²åŒºå†…å®¹
                            if reasoning_buffer:
                                yield {
                                    "type": "reasoning_chunk",
                                    "content": reasoning_buffer,
                                    "done": False
                                }
                                reasoning_buffer = ""
                                reasoning_buffer_size = 0
                            
                            # æ ‡è®°æ¨ç†å®Œæˆ
                            yield {
                                "type": "reasoning_chunk",
                                "content": "",
                                "done": True,
                                "full_reasoning": reasoning_content,
                                "need_requery": False,
                                "new_query": None,
                                "context_summary": "",
                                "requery_depth": 0
                            }
                            reasoning_done = True
                            logger.info(f"âœ… DeepSeek Reasoner æ¨ç†å®Œæˆ: {len(reasoning_content)} å­—ç¬¦")
                        
                        answer_part = delta.content
                        answer_content += answer_part
                        answer_buffer += answer_part
                        answer_buffer_size += len(answer_part)
                        answer_chunk_count += 1
                        
                        # ç­”æ¡ˆç¼“å†²è¾“å‡º
                        if answer_buffer_size >= 30 or answer_chunk_count % 3 == 0:
                            yield {
                                "type": "answer_chunk",
                                "content": answer_buffer,
                                "done": False
                            }
                            answer_buffer = ""
                            answer_buffer_size = 0
                            await asyncio.sleep(0.01)
                
                # å‘é€å‰©ä½™å†…å®¹
                if reasoning_buffer:
                    yield {
                        "type": "reasoning_chunk",
                        "content": reasoning_buffer,
                        "done": False
                    }
                
                if answer_buffer:
                    yield {
                        "type": "answer_chunk",
                        "content": answer_buffer,
                        "done": False
                    }
                
                # å¦‚æœæœ‰æ¨ç†å†…å®¹ä½†è¿˜æ²¡æ ‡è®°å®Œæˆï¼Œæ ‡è®°æ¨ç†å®Œæˆ
                if reasoning_content and not reasoning_done:
                    yield {
                        "type": "reasoning_chunk",
                        "content": "",
                        "done": True,
                        "full_reasoning": reasoning_content,
                        "need_requery": False,
                        "new_query": None,
                        "context_summary": "",
                        "requery_depth": 0
                    }
                    logger.info(f"âœ… DeepSeek Reasoner æ¨ç†å®Œæˆ: {len(reasoning_content)} å­—ç¬¦")
                
                # ç­”æ¡ˆç”Ÿæˆå®Œæˆ
                logger.info(f"âœ… DeepSeek Reasoner ç­”æ¡ˆç”Ÿæˆå®Œæˆ: {len(answer_content)} å­—ç¬¦")
                
                yield {
                    "type": "answer_chunk",
                    "content": "",
                    "done": True,
                    "full_answer": answer_content,
                    "full_reasoning": reasoning_content
                }
                
                return  # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåç»­çš„æ·±åº¦æ€è€ƒé€»è¾‘
                
            else:
                logger.info("ğŸ§  å¼€å§‹æ·±åº¦æ€è€ƒæ¨ç†è¿‡ç¨‹...")
                
                # è·å–æ·±åº¦æ€è€ƒå‚æ•°ï¼ˆé»˜è®¤åªè¿›è¡Œ1æ¬¡äºŒæ¬¡æ£€ç´¢ï¼‰
                max_requery_depth = state.get("max_requery_depth", 1)
                current_depth = 0
                reasoning_messages = []
                
                # è¦æ±‚LLMçœ‹å®Œæ•´æ–‡æ¡£åˆ¤æ–­ä¸Šä¸‹æ–‡æ˜¯å¦å……åˆ†
                reasoning_system_prompt = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚

è¯·ä»”ç»†é˜…è¯»ä¸‹é¢æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œåˆ†ææ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**é‡è¦è¯´æ˜ï¼š**
1. **ä¸Šä¸‹æ–‡å±•ç¤ºç­–ç•¥**ï¼šä¸ºäº†æé«˜åˆ†ææ•ˆç‡ï¼Œç³»ç»Ÿå·²ç»æŒ‰ç…§ç›¸å…³æ€§æ’åºï¼Œå±•ç¤ºå‰20ä¸ªæœ€ç›¸å…³çš„å®ä½“ã€å…³ç³»å’Œæ–‡æ¡£ç‰‡æ®µã€‚å¦‚æœè¿™äº›ä¿¡æ¯ä¸è¶³ï¼Œä½ å¯ä»¥è¯·æ±‚è¿›è¡ŒäºŒæ¬¡æ£€ç´¢è·å–æ›´å¤šä¿¡æ¯ã€‚

2. **âš ï¸ æå…¶é‡è¦ï¼šä¸åŒä¿é™©äº§å“çš„æ¡æ¬¾å·®å¼‚**
   - æ¯ä¸ªä¿é™©äº§å“éƒ½æœ‰ç‹¬ç«‹çš„ä¿é™©æ¡æ¬¾ï¼Œä¸åŒäº§å“çš„æ¡æ¬¾å†…å®¹å¯èƒ½å®Œå…¨ä¸åŒ
   - ä¾‹å¦‚ï¼šã€Œä¼ å®¶å®ã€ä¿é™©æ¡æ¬¾å¯èƒ½ä¸å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜ï¼Œè€Œã€Œä¼ å®¶ç¦ã€ä¿é™©æ¡æ¬¾å¯èƒ½å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜
   - **ç»å¯¹ä¸è¦**å°†ä¸€ä¸ªäº§å“çš„æ¡æ¬¾å¥—ç”¨åˆ°å¦ä¸€ä¸ªäº§å“ä¸Š
   - å¦‚æœå½“å‰æ–‡æ¡£å—æ¥è‡ªä¸åŒçš„ä¿é™©äº§å“ï¼Œå¿…é¡»ä¸¥æ ¼åŒºåˆ†å„è‡ªçš„æ¡æ¬¾å†…å®¹
   - å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·è¯¢é—®çš„å…·ä½“äº§å“çš„ä¿¡æ¯ï¼Œåº”è¯¥ç”Ÿæˆæ–°çš„æ£€ç´¢æŸ¥è¯¢æ¥è·å–æ­£ç¡®çš„äº§å“ä¿¡æ¯

3. **æ–‡æ¡£æ ¼å¼è¯´æ˜**ï¼šæ–‡æ¡£å†…å®¹å¯èƒ½åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œå…¶æ ¼å¼å¦‚ä¸‹ï¼š
   - `[SOURCE:__TABLE_ENTITY_X__]` è¡¨ç¤ºè¯¥å—å¯¹åº”çš„è¡¨æ ¼å ä½ç¬¦ï¼ŒXä¸ºè¡¨æ ¼ç¼–å·
   - `[CONTEXT]` åé¢æ˜¯ä¸è¯¥è¡¨æ ¼ç›¸å…³çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªå…¶ä»–è¡¨æ ¼å ä½ç¬¦ï¼Œä½†è¯·åªå…³æ³¨å½“å‰SOURCEå¯¹åº”çš„è¡¨æ ¼
   - `[HTML_TABLE]` åé¢æ˜¯çœŸå®è¡¨æ ¼çš„HTMLå†…å®¹ï¼ŒåŒ…å«äº†è¡¨æ ¼çš„è¯¦ç»†æ•°æ®

åœ¨åˆ†ææ—¶ï¼Œè¯·ç†è§£è¡¨æ ¼å ä½ç¬¦ä¸å®é™…HTMLè¡¨æ ¼çš„å¯¹åº”å…³ç³»ï¼Œä»HTMLè¡¨æ ¼ä¸­æå–å‡†ç¡®çš„æ•°å€¼ã€æ¡æ¬¾ã€è´¹ç‡ç­‰å…³é”®ä¿¡æ¯ã€‚

**ä»»åŠ¡æµç¨‹ï¼š**

1. **ç†è§£é—®é¢˜**ï¼šè¯´æ˜ä½ å¦‚ä½•ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼ˆ1-2å¥è¯ï¼‰

2. **è¯„ä¼°ä¸Šä¸‹æ–‡**ï¼šåˆ¤æ–­å½“å‰æ–‡æ¡£å’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ç­”æ¡ˆä¾æ®ï¼ˆ2-3å¥è¯ï¼‰

3. **å†³ç­–**ï¼š
   - å¦‚æœä¸Šä¸‹æ–‡**å……åˆ†**ï¼šè¯´æ˜ä½ çš„æ¨ç†é€»è¾‘ï¼ˆ2-3å¥è¯ï¼‰ï¼Œå¹¶åœ¨æœ€åä¸€è¡Œè¾“å‡ºï¼š`[CONTEXT_SUFFICIENT]`
   - å¦‚æœä¸Šä¸‹æ–‡**ä¸è¶³**ï¼š
     a) å…ˆæ€»ç»“å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆ2-3å¥è¯ï¼‰
     b) è¯´æ˜è¿˜ç¼ºå°‘ä»€ä¹ˆå…³é”®ä¿¡æ¯ï¼ˆ1-2å¥è¯ï¼‰
     c) åœ¨æœ€åä¸€è¡Œè¾“å‡ºï¼š`[CONTEXT_SUMMARY: ä½ çš„æ€»ç»“] | [NEW_QUERY: ä½ ç”Ÿæˆçš„æ–°æŸ¥è¯¢è¯­å¥]`

**è¦æ±‚ï¼š**
- ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘ç†è§£..."ã€"æˆ‘å‘ç°..."ï¼‰
- ä»”ç»†é˜…è¯»æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œä¸è¦ä»…çœ‹é¢„è§ˆ
- æ€»å­—æ•°200-400å­—
- å¿…é¡»åœ¨æœ€åä¸€è¡Œæ˜ç¡®è¾“å‡ºå†³ç­–æ ‡è®°'''
                
                reasoning_messages.append({"role": "system", "content": reasoning_system_prompt})
                
                # æ·»åŠ å†å²å¯¹è¯
                if chat_history:
                    for msg in chat_history[-4:]:
                        reasoning_messages.append({
                            "role": msg.get("role"),
                            "content": msg.get("content")
                        })
                
                # æä¾›å®Œæ•´çš„ä¸Šä¸‹æ–‡å†…å®¹ç»™LLMåˆ†æ
                reasoning_user_message = f"""**ç”¨æˆ·é—®é¢˜ï¼š** {query}

{full_context}

è¯·ä»”ç»†é˜…è¯»ä¸Šè¿°æ‰€æœ‰å†…å®¹ï¼Œåˆ†æä¸Šä¸‹æ–‡æ˜¯å¦å……åˆ†ï¼Œå¹¶ç»™å‡ºä½ çš„æ¨ç†åˆ†æå’Œå†³ç­–ã€‚"""
                
                reasoning_messages.append({"role": "user", "content": reasoning_user_message})
                
                # æµå¼ç”ŸæˆLLMæ¨ç†éƒ¨åˆ†
                import asyncio
                llm_reasoning = ""
                chunk_count = 0
                buffer = ""  # ç¼“å†²åŒºï¼Œç´¯ç§¯å°chunk
                buffer_size = 0
                
                async for chunk in llm.astream(reasoning_messages):
                    content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                    llm_reasoning += content
                    buffer += content
                    buffer_size += len(content)
                    chunk_count += 1
                    
                    # å½“ç¼“å†²åŒºè¾¾åˆ°ä¸€å®šå¤§å°ï¼ˆä¾‹å¦‚20å­—ç¬¦ï¼‰æˆ–æ¯5ä¸ªchunkæ—¶æ‰yield
                    if buffer_size >= 20 or chunk_count % 5 == 0:
                        yield {
                            "type": "reasoning_chunk",
                            "content": buffer,
                            "done": False
                        }
                        buffer = ""
                        buffer_size = 0
                        # é‡Šæ”¾äº‹ä»¶å¾ªç¯ï¼Œé¿å…é˜»å¡
                        await asyncio.sleep(0.01)
                
                # å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
                if buffer:
                    yield {
                        "type": "reasoning_chunk",
                        "content": buffer,
                        "done": False
                    }
                
                # æ€è€ƒè¿‡ç¨‹å®Œæˆ
                full_reasoning = llm_reasoning
                logger.info(f"âœ… æ€è€ƒæ¨ç†å®Œæˆ: LLMæ¨ç† {len(llm_reasoning)} å­—ç¬¦")
                
                # === åˆ¤æ–­æ˜¯å¦éœ€è¦äºŒæ¬¡æ£€ç´¢ ===
                need_requery = False
                new_query = None
                context_summary = ""
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦äºŒæ¬¡æ£€ç´¢ä¸”æœªè¶…è¿‡æ·±åº¦é™åˆ¶
                if "[CONTEXT_SUMMARY:" in llm_reasoning and "[NEW_QUERY:" in llm_reasoning:
                    if current_depth < max_requery_depth:
                        # æå–æ€»ç»“å’Œæ–°æŸ¥è¯¢
                        import re
                        summary_match = re.search(r'\[CONTEXT_SUMMARY:\s*(.+?)\]\s*\|', llm_reasoning, re.DOTALL)
                        query_match = re.search(r'\[NEW_QUERY:\s*(.+?)\]', llm_reasoning)
                        
                        if summary_match and query_match:
                            context_summary = summary_match.group(1).strip()
                            new_query = query_match.group(1).strip()
                            need_requery = True
                            current_depth += 1
                            
                            logger.info(f"ğŸ”„ LLMåˆ¤æ–­éœ€è¦äºŒæ¬¡æ£€ç´¢ (æ·±åº¦ {current_depth}/{max_requery_depth})")
                            logger.info(f"ğŸ“ å½“å‰ä¸Šä¸‹æ–‡æ€»ç»“: {context_summary[:100]}...")
                            logger.info(f"ğŸ” æ–°æŸ¥è¯¢: {new_query}")
                            
                            # æ˜¾ç¤ºäºŒæ¬¡æ£€ç´¢æç¤º
                            requery_notice = f"\n\n---\n\nğŸ”„ **æ·±åº¦æ€è€ƒï¼šéœ€è¦äºŒæ¬¡æ£€ç´¢** (æ·±åº¦ {current_depth}/{max_requery_depth})\n\n**å½“å‰ä¸Šä¸‹æ–‡æ€»ç»“ï¼š**\n{context_summary}\n\n**ä¼˜åŒ–æŸ¥è¯¢ï¼š** `{new_query}`\n\næ­£åœ¨è¿›è¡ŒäºŒæ¬¡æ£€ç´¢...\n\n"
                            yield {
                                "type": "reasoning_chunk",
                                "content": requery_notice,
                                "done": False
                            }
                            
                            # === æ‰§è¡ŒäºŒæ¬¡æ£€ç´¢ ===
                            logger.info("ğŸ” å¼€å§‹äºŒæ¬¡æ£€ç´¢...")
                            try:
                                # è°ƒç”¨æ£€ç´¢èŠ‚ç‚¹
                                requery_state = {
                                    "query": new_query,
                                    "query_mode": state.get("query_mode", "hybrid")
                                }
                                retrieval_result = await self.retrieve_context(requery_state)
                                
                                # è°ƒç”¨ç²¾æ’èŠ‚ç‚¹
                                rerank_state = {
                                    "query": new_query,
                                    "retrieved_docs": retrieval_result.get("retrieved_docs", []),
                                    "reranker": state.get("reranker"),
                                    "rerank_top_k": state.get("rerank_top_k")
                                }
                                rerank_result = await self.rerank_context(rerank_state)
                                
                                # è·å–æ–°ä¸Šä¸‹æ–‡
                                new_final_docs = rerank_result.get("final_docs", [])
                                new_entities = retrieval_result.get("retrieved_entities", [])
                                new_relationships = retrieval_result.get("retrieved_relationships", [])
                                
                                # æ„å»ºæ–°ä¸Šä¸‹æ–‡
                                new_context = self._build_context(
                                    new_final_docs, new_entities, new_relationships
                                )
                                
                                logger.info(f"âœ… äºŒæ¬¡æ£€ç´¢å®Œæˆ: {len(new_final_docs)} ä¸ªæ–‡æ¡£, {len(new_entities)} ä¸ªå®ä½“, {len(new_relationships)} æ¡å…³ç³»")
                                
                                # æ‹¼æ¥æ€»ç»“ä¸æ–°ä¸Šä¸‹æ–‡
                                full_context = f"""## åŸå§‹ä¸Šä¸‹æ–‡æ€»ç»“

    {context_summary}

    ---

    ## äºŒæ¬¡æ£€ç´¢çš„æ–°å¢ä¸Šä¸‹æ–‡

    {new_context}"""
                                
                                # æ›´æ–°çŠ¶æ€å˜é‡
                                final_docs = new_final_docs
                                retrieved_entities = new_entities
                                retrieved_relationships = new_relationships
                                
                                # æ˜¾ç¤ºäºŒæ¬¡æ£€ç´¢ç»“æœ
                                requery_result_notice = f"\nâœ… **äºŒæ¬¡æ£€ç´¢å®Œæˆ**\n- æ–°å¢æ–‡æ¡£: {len(new_final_docs)} ä¸ª\n- æ–°å¢å®ä½“: {len(new_entities)} ä¸ª\n- æ–°å¢å…³ç³»: {len(new_relationships)} æ¡\n\nå·²å°†åŸå§‹æ€»ç»“ä¸æ–°ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼Œæ­£åœ¨ç”Ÿæˆå¢å¼ºå›ç­”...\n\n"
                                yield {
                                    "type": "reasoning_chunk",
                                    "content": requery_result_notice,
                                    "done": False
                                }
                                
                                # æ›´æ–°reasoningè®°å½•
                                full_reasoning += requery_notice + requery_result_notice
                                
                            except Exception as e:
                                logger.error(f"âŒ äºŒæ¬¡æ£€ç´¢å¤±è´¥: {e}")
                                error_notice = f"\n\nâš ï¸ äºŒæ¬¡æ£€ç´¢å¤±è´¥: {str(e)}\nå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆã€‚\n\n"
                                yield {
                                    "type": "reasoning_chunk",
                                    "content": error_notice,
                                    "done": False
                                }
                                full_reasoning += error_notice
                    else:
                        logger.info(f"âš ï¸  å·²è¾¾æœ€å¤§æ£€ç´¢æ·±åº¦ ({max_requery_depth})ï¼Œä¸å†è¿›è¡ŒäºŒæ¬¡æ£€ç´¢")
                        depth_limit_notice = f"\n\nâš ï¸  å·²è¾¾æœ€å¤§æ·±åº¦æ€è€ƒæ¬¡æ•° ({max_requery_depth})ï¼Œå°†åŸºäºå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆã€‚\n\n"
                        yield {
                            "type": "reasoning_chunk",
                            "content": depth_limit_notice,
                            "done": False
                        }
                        full_reasoning += depth_limit_notice
                
                # æ ‡è®°æ¨ç†å®Œæˆ
                yield {
                    "type": "reasoning_chunk",
                    "content": "",
                    "done": True,
                    "full_reasoning": full_reasoning,
                    "need_requery": need_requery,
                    "new_query": new_query,
                    "context_summary": context_summary,
                    "requery_depth": current_depth
                }
                
            # === ç¬¬äºŒæ­¥ï¼šåŸºäºæ€è€ƒè¿‡ç¨‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ===
            logger.info("ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
            logger.info(f"   - å®ä½“: {len(retrieved_entities)} ä¸ª")
            logger.info(f"   - å…³ç³»: {len(retrieved_relationships)} æ¡")
            logger.info(f"   - æ–‡æ¡£: {len(final_docs)} ä¸ª")
            
            # é‡æ–°æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰ï¼Œä¾›ç­”æ¡ˆç”Ÿæˆä½¿ç”¨
            # æ³¨æ„ï¼šå¦‚æœè¿›è¡Œäº†äºŒæ¬¡æ£€ç´¢ï¼Œfull_contextå·²ç»åœ¨ä¸Šé¢è¢«æ›´æ–°ä¸ºåŒ…å«æ–°ä¸Šä¸‹æ–‡çš„ç‰ˆæœ¬
            if not need_requery:
                # å¦‚æœæ²¡æœ‰è¿›è¡ŒäºŒæ¬¡æ£€ç´¢ï¼Œéœ€è¦é‡æ–°æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆæ·±åº¦æ€è€ƒé˜¶æ®µåªç”¨äº†å‰20ä¸ªï¼‰
                full_context = self._build_context(
                    final_docs, retrieved_entities, retrieved_relationships, max_items=None
                )
                logger.info("ğŸ“ å·²é‡æ–°æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆæ— é™åˆ¶ï¼‰ç”¨äºç­”æ¡ˆç”Ÿæˆ")
            
            answer_messages = []
            
            # ç­”æ¡ˆç”Ÿæˆçš„ç³»ç»Ÿæç¤º
            answer_system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä¸‹é¢æä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†å›¾è°±åŒ…å«äº†ä»æ–‡æ¡£ä¸­æå–çš„å®ä½“å’Œå…³ç³»ï¼Œæä¾›äº†ç»“æ„åŒ–çš„çŸ¥è¯†è§†å›¾ã€‚
æ–‡æ¡£å†…å®¹æ˜¯ç»è¿‡ç²¾æ’çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚

**âš ï¸ æå…¶é‡è¦ï¼šä¸åŒä¿é™©äº§å“çš„æ¡æ¬¾å·®å¼‚**
1. **äº§å“ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªä¿é™©äº§å“éƒ½æœ‰å…¶ç‹¬ç«‹çš„ä¿é™©æ¡æ¬¾ï¼Œä¸åŒäº§å“çš„æ¡æ¬¾å†…å®¹å¯èƒ½å®Œå…¨ä¸åŒã€‚
   - ä¾‹å¦‚ï¼šã€Œä¼ å®¶å®ã€ä¿é™©æ¡æ¬¾å¯èƒ½ä¸å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜
   - è€Œã€Œä¼ å®¶ç¦ã€ä¿é™©æ¡æ¬¾å¯èƒ½å¯¹å…¨æ®‹è¿›è¡Œèµ”ä»˜
   
2. **ä¸¥æ ¼åŒ¹é…**ï¼šå›ç­”é—®é¢˜æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼åŒºåˆ†ä¸åŒçš„ä¿é™©äº§å“ï¼š
   - æ£€æŸ¥æ–‡æ¡£æ¥æºï¼ˆfile_pathã€chunk_idï¼‰ç¡®è®¤äº§å“åç§°
   - ä¸è¦å°†ä¸€ä¸ªäº§å“çš„æ¡æ¬¾å¥—ç”¨åˆ°å¦ä¸€ä¸ªäº§å“ä¸Š
   - å¦‚æœç”¨æˆ·è¯¢é—®çš„æ˜¯äº§å“Aï¼Œç»ä¸èƒ½ç”¨äº§å“Bçš„æ¡æ¬¾æ¥å›ç­”
   
3. **ä¿¡æ¯ä¸è¶³æ—¶çš„å¤„ç†**ï¼š
   - å¦‚æœå½“å‰ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·è¯¢é—®çš„å…·ä½“äº§å“çš„ç›¸å…³ä¿¡æ¯
   - æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"å½“å‰æ£€ç´¢çš„ä¿¡æ¯ä¸åŒ…å«è¯¥äº§å“çš„ç›¸å…³æ¡æ¬¾"
   - ä¸è¦çŒœæµ‹æˆ–ä½¿ç”¨å…¶ä»–äº§å“çš„ä¿¡æ¯è¿›è¡Œæ¨æ–­

**æ–‡æ¡£æ ¼å¼è¯´æ˜**
æ–‡æ¡£å†…å®¹å¯èƒ½åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œå…¶æ ¼å¼å¦‚ä¸‹ï¼š
- `[SOURCE:__TABLE_ENTITY_X__]` è¡¨ç¤ºè¯¥å—å¯¹åº”çš„è¡¨æ ¼å ä½ç¬¦ï¼ŒXä¸ºè¡¨æ ¼ç¼–å·
- `[CONTEXT]` åé¢æ˜¯ä¸è¯¥è¡¨æ ¼ç›¸å…³çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªå…¶ä»–è¡¨æ ¼å ä½ç¬¦ï¼Œä½†è¯·åªå…³æ³¨å½“å‰SOURCEå¯¹åº”çš„è¡¨æ ¼
- `[HTML_TABLE]` åé¢æ˜¯çœŸå®è¡¨æ ¼çš„HTMLå†…å®¹ï¼ŒåŒ…å«äº†è¡¨æ ¼çš„è¯¦ç»†æ•°æ®

åœ¨å›ç­”æ—¶ï¼Œè¯·ç†è§£è¡¨æ ¼å ä½ç¬¦ä¸å®é™…HTMLè¡¨æ ¼çš„å¯¹åº”å…³ç³»ï¼Œä»HTMLè¡¨æ ¼ä¸­æå–å‡†ç¡®çš„æ•°å€¼ã€æ¡æ¬¾ã€è´¹ç‡ç­‰å…³é”®ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§ã€‚

**ç”Ÿæˆç­”æ¡ˆçš„è¦æ±‚ï¼š**
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ç†è§£å®ä½“é—´çš„å…³ç³»
2. ç»“åˆæ–‡æ¡£å†…å®¹ï¼ˆåŒ…æ‹¬è¡¨æ ¼æ•°æ®ï¼‰æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ”¯æŒ
3. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„å®ä½“ã€å…³ç³»æˆ–æ–‡æ¡£æ¥æº
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥
6. **é‡è¦**ï¼šä½ çš„åˆ†ææ€è·¯å·²ç»åœ¨å‰é¢å±•ç¤ºè¿‡äº†ï¼Œè¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦é‡å¤æ€è€ƒè¿‡ç¨‹çš„å†…å®¹'''
            
            answer_messages.append({"role": "system", "content": answer_system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
            if chat_history:
                for msg in chat_history[-10:]:
                    answer_messages.append({
                        "role": msg.get("role"),
                        "content": msg.get("content")
                    })
            
            # æ·»åŠ åˆšæ‰çš„æ€è€ƒè¿‡ç¨‹ä½œä¸ºä¸Šä¸‹æ–‡
            answer_messages.append({
                "role": "assistant",
                "content": f"ã€æˆ‘çš„åˆ†ææ€è·¯ã€‘\n{full_reasoning}"
            })
            
            # æ·»åŠ å½“å‰æŸ¥è¯¢ï¼ˆåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
            answer_user_message = f"""è¯·åŸºäºä»¥ä¸‹ä¿é™©çŸ¥è¯†åº“ä¿¡æ¯å’Œä½ çš„åˆ†ææ€è·¯ï¼Œç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆ:

{full_context}

**ç”¨æˆ·é—®é¢˜:** {query}"""
            
            answer_messages.append({"role": "user", "content": answer_user_message})
            
            # æµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            full_answer = ""
            answer_buffer = ""
            answer_buffer_size = 0
            answer_chunk_count = 0
            
            async for chunk in llm.astream(answer_messages):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += content
                answer_buffer += content
                answer_buffer_size += len(content)
                answer_chunk_count += 1
                
                # å½“ç¼“å†²åŒºè¾¾åˆ°ä¸€å®šå¤§å°ï¼ˆä¾‹å¦‚30å­—ç¬¦ï¼‰æˆ–æ¯3ä¸ªchunkæ—¶æ‰yield
                if answer_buffer_size >= 30 or answer_chunk_count % 3 == 0:
                    yield {
                        "type": "answer_chunk",
                        "content": answer_buffer,
                        "done": False
                    }
                    answer_buffer = ""
                    answer_buffer_size = 0
                    # é‡Šæ”¾äº‹ä»¶å¾ªç¯ï¼Œé¿å…é˜»å¡
                    await asyncio.sleep(0.01)
            
            # å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
            if answer_buffer:
                yield {
                    "type": "answer_chunk",
                    "content": answer_buffer,
                    "done": False
                }
            
            # æ ‡è®°å®Œæˆ
            logger.info(f"âœ… æµå¼ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(full_answer)} å­—ç¬¦)")
            yield {
                "type": "answer_chunk",
                "content": "",
                "done": True,
                "full_answer": full_answer,
                "full_reasoning": full_reasoning
            }
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield {
                "type": "answer_chunk",
                "content": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                "done": True
            }
    
    def _build_context(self, final_docs, retrieved_entities, retrieved_relationships, max_items=None):
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆä¾›æµå¼å’Œéæµå¼ç‰ˆæœ¬å…±ç”¨ï¼‰
        
        Args:
            final_docs: æ–‡æ¡£åˆ—è¡¨
            retrieved_entities: å®ä½“åˆ—è¡¨
            retrieved_relationships: å…³ç³»åˆ—è¡¨
            max_items: æœ€å¤§é¡¹ç›®æ•°é‡é™åˆ¶ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶ã€‚ç”¨äºæ·±åº¦æ€è€ƒé˜¶æ®µå‡å°‘ä¿¡æ¯é‡
        """
        kg_context_parts = []

        if retrieved_entities:
            entity_context = "### ç›¸å…³å®ä½“\n\n"
            # é™åˆ¶å®ä½“æ•°é‡
            entities_to_show = retrieved_entities[:max_items] if max_items else retrieved_entities
            total_entities = len(retrieved_entities)
            
            for idx, entity in enumerate(entities_to_show, 1):
                entity_name = entity.get('entity_name', 'æœªçŸ¥')
                entity_type = entity.get('entity_type', 'æœªçŸ¥')
                description = entity.get('description', 'æ— æè¿°')
                entity_context += f"{idx}. **{entity_name}** ({entity_type})\n   {description}\n\n"
            
            # å¦‚æœæœ‰æˆªæ–­ï¼Œæç¤ºç”¨æˆ·
            if max_items and total_entities > max_items:
                entity_context += f"*ï¼ˆå…±{total_entities}ä¸ªå®ä½“ï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰{max_items}ä¸ªæœ€ç›¸å…³çš„ï¼‰*\n\n"
            
            kg_context_parts.append(entity_context)

        if retrieved_relationships:
            relation_context = "### ç›¸å…³å…³ç³»\n\n"
            # é™åˆ¶å…³ç³»æ•°é‡
            relationships_to_show = retrieved_relationships[:max_items] if max_items else retrieved_relationships
            total_relationships = len(retrieved_relationships)
            
            for idx, rel in enumerate(relationships_to_show, 1):
                src = rel.get('src_id', '?')
                tgt = rel.get('tgt_id', '?')
                desc = rel.get('description', 'æ— æè¿°')
                weight = rel.get('weight', 0)
                relation_context += f"{idx}. {src} â†’ {tgt} (æƒé‡: {weight:.2f})\n   {desc}\n\n"
            
            # å¦‚æœæœ‰æˆªæ–­ï¼Œæç¤ºç”¨æˆ·
            if max_items and total_relationships > max_items:
                relation_context += f"*ï¼ˆå…±{total_relationships}æ¡å…³ç³»ï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰{max_items}æ¡æœ€ç›¸å…³çš„ï¼‰*\n\n"
            
            kg_context_parts.append(relation_context)

        doc_context_parts = []
        if final_docs:
            doc_context_parts.append("### ç›¸å…³æ–‡æ¡£\n")
            # é™åˆ¶æ–‡æ¡£æ•°é‡
            docs_to_show = final_docs[:max_items] if max_items else final_docs
            total_docs = len(final_docs)
            
            for idx, doc in enumerate(docs_to_show, 1):
                rerank_score = doc.metadata.get('rerank_score', 'N/A')
                score_str = f"{rerank_score:.4f}" if isinstance(rerank_score, float) else str(rerank_score)
                chunk_id = doc.metadata.get('chunk_id', 'æœªçŸ¥')
                file_path = doc.metadata.get('file_path', 'æœªçŸ¥')

                doc_context_parts.append(
                    f"ã€æ–‡æ¡£ {idx}ã€‘\n"
                    f"Chunk ID: {chunk_id}\n"
                    f"æ¥æº: {file_path}\n"
                    f"ç½®ä¿¡åº¦: {score_str}\n"
                    f"å†…å®¹:\n{doc.page_content}\n"
                )
            
            # å¦‚æœæœ‰æˆªæ–­ï¼Œæç¤ºç”¨æˆ·
            if max_items and total_docs > max_items:
                doc_context_parts.append(f"\n*ï¼ˆå…±{total_docs}ä¸ªæ–‡æ¡£ï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰{max_items}ä¸ªæœ€ç›¸å…³çš„ï¼‰*\n")

        kg_context_str = "\n".join(kg_context_parts) if kg_context_parts else ""
        doc_context_str = "\n" + ("-" * 60 + "\n").join(doc_context_parts) if doc_context_parts else ""

        full_context = ""
        if kg_context_str:
            full_context += "## çŸ¥è¯†å›¾è°±ä¿¡æ¯\n\n" + kg_context_str + "\n"
        if doc_context_str:
            full_context += "## æ–‡æ¡£å†…å®¹\n" + doc_context_str

        return full_context