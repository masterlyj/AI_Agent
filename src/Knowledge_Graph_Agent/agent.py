import asyncio
import os
from typing import List, Optional, Dict
from dotenv import load_dotenv

#å¯¼å…¥ Reranker æ¨¡å—
from .reranker import RerankerModel
from .light_graph_rag import LightRAG
from .nodes import WorkflowNodes
from .graph import create_indexing_graph, create_querying_graph
from .state import IndexingState
from .utils import logger
from .kg.shared_storage import initialize_pipeline_status
from .llm import get_llm
from .async_lanchain_rag_adapter import create_lightrag_compatible_complete
from .embedding_factory import get_embedder, create_lightrag_embedding_adapter
from .mineru_integration import SmartDocumentIndexer

#åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- RAGAgent with Real LLM and Embedding ---
class RAGAgent:
    def __init__(self, working_dir: str):
        self.working_dir = working_dir
        self.rag: Optional[LightRAG] = None
        self.nodes: Optional[WorkflowNodes] = None
        self.indexing_graph = None
        self.querying_graph = None
        self.smart_indexer: Optional[SmartDocumentIndexer] = None
        self.reranker: Optional[RerankerModel] = None
        self.langchain_llm = None
        self.model_name = None  # ä¿å­˜æ¨¡å‹åç§°ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æ·±åº¦æ€è€ƒ
        self.rerank_top_k = 20

    @classmethod
    async def create(cls, working_dir: str = None, rerank_config: dict = None, storage_mode: str = "database"):
        """
        å¼‚æ­¥å·¥å‚æ–¹æ³•ï¼Œåˆ›å»ºå¹¶åˆå§‹åŒ– RAGAgent å®ä¾‹ã€‚
        
        Args:
            working_dir: å·¥ä½œç›®å½•
            rerank_config: Rerankeré…ç½®
            storage_mode: å­˜å‚¨æ¨¡å¼ ("database" æˆ– "memory")
        """
        instance = cls(working_dir or os.getenv("WORKING_DIR", "data/rag_storage"))
        
        # å¦‚æœæœªæä¾›rerank_configï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        if rerank_config is None:
            rerank_config = {}
            rerank_config['model_path'] = os.getenv("RERANK_MODEL_PATH", "maidalun1020/bce-reranker-base_v1")
            rerank_config['device'] = os.getenv("RERANK_DEVICE", "cpu")
            rerank_config['top_k'] = int(os.getenv("RERANK_TOP_K", "20"))
            rerank_config['use_fp16'] = os.getenv("RERANK_USE_FP16", "false").lower() == "true"
        
        os.makedirs(working_dir, exist_ok=True)
        
        # === 1. è·å– LangChain LLM å®ä¾‹å’Œæ¨¡å‹åç§° ===
        langchain_llm, model_name = get_llm() 
        
        # LLM å®ä¾‹
        instance.langchain_llm = langchain_llm
        instance.model_name = model_name
        logger.info(f"âœ… LangChain LLM å·²åŠ è½½ç”¨äºé—®ç­”ç”Ÿæˆ")
        
        # === 2. åŒ…è£…ä¸º LightRAG å…¼å®¹çš„å¼‚æ­¥å‡½æ•° ===
        llm_func = create_lightrag_compatible_complete(
            langchain_llm,
            retry_attempts=3,
            retry_min_wait=4
        )
        
        # === 3. è·å–åµŒå…¥æ¨¡å‹ï¼ˆä» .env æ„å»ºé…ç½®ï¼‰ ===
        etype = os.getenv("EMBEDDING_TYPE", "ollama").strip()
        
        if etype == "hf":
            emb_model_name = os.getenv("HF_EMBEDDING_MODEL_NAME", "BAAI/bge-m3").strip()
            model_kwargs = {}
            device = os.getenv("HF_EMBEDDING_DEVICE", "").strip()
            if device:
                model_kwargs["device"] = device
            # å¯é€‰é™„åŠ å‚æ•°
            if os.getenv("HF_EMBEDDING_TRUST_REMOTE_CODE", "false").lower() == "true":
                model_kwargs["trust_remote_code"] = True
            
            embedding_config = {
                "type": "hf",
                "model_name": emb_model_name,
                "model_kwargs": model_kwargs,
                "encode_kwargs": {},
                "show_progress": os.getenv("HF_EMBEDDING_SHOW_PROGRESS", "false").lower() == "true",
                "multi_process": os.getenv("HF_EMBEDDING_MULTI_PROCESS", "false").lower() == "true",
            }
            logger.info(f"âœ… é…ç½® HuggingFace Embedding: {emb_model_name}")
        
        elif etype == "ollama":
            embedding_config = {
                "type": "ollama",
                "model": os.getenv("OLLAMA_EMBEDDING_MODEL", "qwen3-embedding:0.6b").strip(),
                "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").strip(),
            }
            logger.info(f"âœ… é…ç½® Ollama Embedding: {embedding_config['model']}")
        
        elif etype == "vllm":
            base_url = os.getenv("VLLM_BASE_URL")
            if not base_url:
                raise ValueError("EMBEDDING_TYPE=vllm ä½†æœªé…ç½® VLLM_BASE_URL")
            embedding_config = {
                "type": "vllm",
                "model": os.getenv("VLLM_EMBEDDING_MODEL", "text-embedding-3-large").strip(),
                "base_url": base_url.strip(),
                "api_key": os.getenv("VLLM_API_KEY", "EMPTY"),
            }
            logger.info(f"âœ… é…ç½® vLLM Embedding: {embedding_config['model']}")
        else:
            raise ValueError(f"æœªçŸ¥ EMBEDDING_TYPE: {etype}ï¼Œæ”¯æŒ: hf, ollama, vllm")
        
        # åˆ›å»º LangChain åµŒå…¥æ¨¡å‹å®ä¾‹
        langchain_embedder = get_embedder(embedding_config)
        
        # ä» .env è¯»å– embedding ç»´åº¦
        embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))
        
        # é€‚é…ä¸º LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
        embedding_func = create_lightrag_embedding_adapter(
            langchain_embedder,
            embedding_dim=embedding_dim
        )
        
        # === 4. åˆå§‹åŒ– Reranker æ¨¡å‹ï¼ˆ.env ä¸ºé»˜è®¤ï¼Œå…¥å‚å¯è¦†ç›–ï¼‰ ===
        env_enabled = os.getenv("RERANK_ENABLED", "false").lower() == "true"
        cfg = rerank_config or {}
        enabled = cfg.get("enabled", env_enabled)
        
        if enabled:
            logger.info("ğŸ”§ åˆå§‹åŒ– Reranker æ¨¡å‹...")
            try:
                # è·å– rerank ç±»å‹ï¼Œé»˜è®¤ä¸º local
                rerank_type = cfg.get("type", os.getenv("RERANK_TYPE", "local").strip().lower())
                top_k = int(cfg.get("top_k", os.getenv("RERANK_TOP_K", "20")))
                
                if rerank_type == "vllm":
                    # ä½¿ç”¨ VLLM Reranker
                    base_url = cfg.get("base_url", os.getenv("RERANK_BASE_URL", "").strip())
                    model = cfg.get("model", os.getenv("RERANK_MODEL", "Qwen3-Reranker-0.6B").strip())
                    api_key = cfg.get("api_key", os.getenv("RERANK_API_KEY", "EMPTY").strip())
                    timeout = int(cfg.get("timeout", os.getenv("RERANK_TIMEOUT", "60")))
                    
                    if not base_url:
                        logger.error("ä½¿ç”¨ VLLM Reranker æ—¶å¿…é¡»é…ç½® RERANK_BASE_URL")
                        instance.reranker = None
                    else:
                        from .reranker import VLLMRerankerModel
                        instance.reranker = VLLMRerankerModel(
                            base_url=base_url,
                            model=model,
                            api_key=api_key,
                            top_k=top_k,
                            timeout=timeout
                        )
                        instance.rerank_top_k = top_k
                        logger.info(f"âœ… VLLM Reranker æ¨¡å‹åŠ è½½å®Œæˆ (model={model}, base_url={base_url}, top_k={instance.rerank_top_k})")
                else:
                    # ä½¿ç”¨æœ¬åœ° Reranker (é»˜è®¤)
                    model = cfg.get("model", os.getenv("RERANK_MODEL", "maidalun1020/bce-reranker-base_v1").strip())
                    device = cfg.get("device", os.getenv("RERANK_DEVICE", "").strip() or None)
                    use_fp16 = cfg.get("use_fp16", os.getenv("RERANK_USE_FP16", "false").lower() == "true")
                    
                    instance.reranker = RerankerModel(
                        model_name_or_path=model,
                        device=device,
                        top_k=top_k,
                        use_fp16=use_fp16,
                    )
                    instance.rerank_top_k = top_k
                    logger.info(f"âœ… æœ¬åœ° Reranker æ¨¡å‹åŠ è½½å®Œæˆ (model={model}, top_k={instance.rerank_top_k})")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ Reranker æ¨¡å‹å¤±è´¥: {e}")
                instance.reranker = None
        
        # === 5. åˆ›å»º LightRAG å®ä¾‹ ===
        # æ ¹æ®storage_modeå‚æ•°é€‰æ‹©ä¸åŒçš„å­˜å‚¨æ–¹å¼
        if storage_mode == "memory":
            # å†…å­˜ç®¡ç†æ¨¡å¼ - ä½¿ç”¨æœ¬åœ°JSONæ–‡ä»¶å­˜å‚¨
            logger.info("ğŸ“ ä½¿ç”¨å†…å­˜ç®¡ç†æ¨¡å¼ - æœ¬åœ°JSONæ–‡ä»¶å­˜å‚¨")
            instance.rag = LightRAG( 
                working_dir=working_dir,
                embedding_func=embedding_func,
                llm_model_func=llm_func,
                kv_storage="JsonKVStorage",
                vector_storage="NanoVectorDBStorage",
                graph_storage="NetworkXStorage",
                doc_status_storage="JsonDocStatusStorage"
            )
        else:
            # æ•°æ®åº“å­˜å‚¨æ¨¡å¼ - ä½¿ç”¨PostgreSQLå­˜å‚¨ï¼ˆé»˜è®¤ï¼‰
            logger.info("ğŸ—„ï¸ ä½¿ç”¨æ•°æ®åº“å­˜å‚¨æ¨¡å¼ - PostgreSQLå­˜å‚¨")
            instance.rag = LightRAG( 
                working_dir=working_dir,
                embedding_func=embedding_func,
                llm_model_func=llm_func,
                kv_storage="PGKVStorage",
                vector_storage="PGVectorStorage",
                graph_storage="PGGraphStorage",
                doc_status_storage="PGDocStatusStorage"
            )
        
        # === 6. åˆå§‹åŒ–å­˜å‚¨å’Œæµæ°´çº¿ ===
        await instance.rag.initialize_storages()
        await initialize_pipeline_status()
        
        # === 7. åˆå§‹åŒ–å·¥ä½œæµ ===
        instance.nodes = WorkflowNodes(instance.rag)
        instance.indexing_graph = create_indexing_graph(instance.nodes)
        instance.querying_graph = create_querying_graph(instance.nodes)
        
        # === 8. åˆå§‹åŒ–æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨ ===
        # ä»ç¯å¢ƒå˜é‡è·å–MinerU APIå¯†é’¥
        mineru_api_key = os.environ.get("MINERU_API_KEY", "")
        instance.smart_indexer = SmartDocumentIndexer(mineru_api_key=mineru_api_key)
        
        return instance

    async def index_documents(self, file_paths: List[str]):
        """æ™ºèƒ½ç´¢å¼•æ–‡æ¡£ - æ”¯æŒPDFå’Œæ–‡æœ¬æ–‡ä»¶"""
        logger.info(f"ğŸ“š å¼€å§‹æ™ºèƒ½ç´¢å¼• {len(file_paths)} ä¸ªæ–‡æ¡£...")
        
        # ä½¿ç”¨æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨å¤„ç†æ–‡ä»¶
        if self.smart_indexer:
            process_result = await self.smart_indexer.process_files_for_indexing(file_paths)
            files_to_index = process_result["files_to_index"]
            
            if not files_to_index:
                logger.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶")
                return {
                    "track_id": None,
                    "status_message": "æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶",
                    "processing_summary": self.smart_indexer.get_processing_summary(process_result)
                }
            
            logger.info(f"ğŸ“„ å‡†å¤‡ç´¢å¼• {len(files_to_index)} ä¸ªå¤„ç†åçš„æ–‡ä»¶")
        else:
            # å¦‚æœæ²¡æœ‰æ™ºèƒ½ç´¢å¼•å™¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶
            files_to_index = file_paths
        
        # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œç´¢å¼•
        contents, ids, paths = [], [], []
        for fp in files_to_index:
            try:
                with open(fp, 'r', encoding='utf-8') as f:
                    contents.append(f.read())
                ids.append(os.path.basename(fp))
                paths.append(os.path.abspath(fp))
                logger.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {os.path.basename(fp)}")
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {fp}: {e}")
                continue
        
        if not contents:
            logger.error("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶å†…å®¹")
            return {
                "track_id": None,
                "status_message": "æ–‡ä»¶è¯»å–å¤±è´¥",
                "processing_summary": "æ–‡ä»¶è¯»å–å¤±è´¥"
            }

        initial_state: IndexingState = {
            "working_dir": self.working_dir,
            "inputs": contents,
            "ids": ids,
            "file_paths": paths,
            "track_id": None,
            "status_message": ""
        }

        result = await self.indexing_graph.ainvoke(initial_state)
        
        # æ·»åŠ å¤„ç†æ‘˜è¦åˆ°ç»“æœä¸­
        if self.smart_indexer and 'processing_summary' not in result:
            result['processing_summary'] = self.smart_indexer.get_processing_summary(process_result)
        
        logger.info(f"ğŸ“Œ ç´¢å¼•æµç¨‹ç»“æŸ: {result['status_message']}")
        return result

    async def query(
        self, 
        question: str, 
        mode: str = "mix", 
        enable_rerank: bool = True,
        rerank_top_k: Optional[int] = None,
        chat_history: List[Dict] = None,
        thread_id: str = None
    ):
        """é€šè¿‡ LangGraph æŸ¥è¯¢æµç¨‹æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (naive, local, global, hybrid, mix)
            enable_rerank: æ˜¯å¦å¯ç”¨ç²¾æ’
            rerank_top_k: ç²¾æ’æ•°é‡
            chat_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            thread_id: ä¼šè¯æ ‡è¯†ï¼ˆå¯é€‰ï¼Œç”¨äºä¼šè¯ç®¡ç†ï¼‰
        
        Returns:
            åŒ…å« context, answer, chat_history çš„å­—å…¸
        """
        from src.Knowledge_Graph_Agent.state import QueryState

        # å¦‚æœæœªæä¾› thread_idï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶ ID
        if thread_id is None:
            import uuid
            thread_id = str(uuid.uuid4())

        initial_query_state: QueryState = {
            "thread_id": thread_id,  # ä¼ å…¥ä¼šè¯æ ‡è¯†
            "working_dir": self.working_dir,
            "query": question,
            "query_mode": mode,
            "llm": self.langchain_llm,
            "model_name": self.model_name,  # ä¼ é€’æ¨¡å‹åç§°
            "reranker": self.reranker if enable_rerank else None,
            "rerank_top_k": rerank_top_k if rerank_top_k is not None else self.rerank_top_k,
            "chat_history": chat_history or [],
            "retrieved_docs": [],
            "retrieved_entities": [],
            "retrieved_relationships": [],
            "final_docs": [],
            "context": {},
            "answer": ""
        }

        # é€šè¿‡ config ä¼ é€’ thread_idï¼ˆç”¨äº LangGraph å†…éƒ¨è¿½è¸ªï¼‰
        config = {"configurable": {"thread_id": thread_id}}

        # ainvoke æ˜¯ç‹¬ç«‹æ‰§è¡Œï¼ŒçŠ¶æ€ä¸ä¼šè·¨è°ƒç”¨ä¿ç•™
        result = await self.querying_graph.ainvoke(
            initial_query_state,
            config=config  # å¯ç”¨äºæ£€æŸ¥ç‚¹/æŒä¹…åŒ–
        )

        logger.info(f"ğŸ” æŸ¥è¯¢æµç¨‹å®Œæˆ (thread_id={thread_id[:8]}..., mode={mode})")

        return {
            "answer": result.get("answer", ""),
            "context": result.get("context", {}),
            "chat_history": result.get("chat_history", [])
        }
    
    async def query_stream(
        self, 
        question: str, 
        mode: str = "mix", 
        enable_rerank: bool = True,
        rerank_top_k: Optional[int] = None,
        chat_history: List[Dict] = None,
        thread_id: str = None
    ):
        """é€šè¿‡æµå¼è¾“å‡ºæŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (naive, local, global, hybrid, mix)
            enable_rerank: æ˜¯å¦å¯ç”¨ç²¾æ’
            rerank_top_k: ç²¾æ’æ•°é‡
            chat_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            thread_id: ä¼šè¯æ ‡è¯†ï¼ˆå¯é€‰ï¼Œç”¨äºä¼šè¯ç®¡ç†ï¼‰
        
        Yields:
            åŒ…å«æµå¼æ›´æ–°çš„å­—å…¸
        """
        from src.Knowledge_Graph_Agent.state import QueryState

        # å¦‚æœæœªæä¾› thread_idï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶ ID
        if thread_id is None:
            import uuid
            thread_id = str(uuid.uuid4())

        logger.info(f"ğŸ” å¼€å§‹æµå¼æŸ¥è¯¢ (thread_id={thread_id[:8]}..., mode={mode})")
        
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        initial_query_state: QueryState = {
            "thread_id": thread_id,
            "working_dir": self.working_dir,
            "query": question,
            "query_mode": mode,
            "llm": self.langchain_llm,
            "model_name": self.model_name,  # ä¼ é€’æ¨¡å‹åç§°
            "reranker": self.reranker if enable_rerank else None,
            "rerank_top_k": rerank_top_k if rerank_top_k is not None else self.rerank_top_k,
            "chat_history": chat_history or [],
            "retrieved_docs": [],
            "retrieved_entities": [],
            "retrieved_relationships": [],
            "final_docs": [],
            "context": {},
            "answer": ""
        }

        try:
            # æ­¥éª¤1: æ‰§è¡Œæ£€ç´¢å’Œç²¾æ’
            yield {"type": "status", "content": "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."}
            
            # æ‰§è¡Œæ£€ç´¢
            retrieve_result = await self.nodes.retrieve_context(initial_query_state)
            initial_query_state.update(retrieve_result)
            
            yield {"type": "status", "content": f"æ£€ç´¢åˆ° {len(retrieve_result.get('retrieved_docs', []))} ä¸ªæ–‡æ¡£"}
            
            # æ‰§è¡Œç²¾æ’
            if initial_query_state.get("reranker"):
                yield {"type": "status", "content": "æ­£åœ¨å¯¹æ–‡æ¡£è¿›è¡Œç²¾æ’..."}
                rerank_result = await self.nodes.rerank_context(initial_query_state)
                initial_query_state.update(rerank_result)
                yield {"type": "status", "content": f"ç²¾æ’å®Œæˆï¼Œé€‰å– Top {len(rerank_result.get('final_docs', []))} æ–‡æ¡£"}
            else:
                # å¦‚æœæ²¡æœ‰rerankerï¼Œç›´æ¥ä½¿ç”¨æ£€ç´¢çš„æ–‡æ¡£
                initial_query_state["final_docs"] = retrieve_result.get("retrieved_docs", [])
            
            # æ­¥éª¤2: æµå¼ç”Ÿæˆç­”æ¡ˆ
            yield {"type": "status", "content": "æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."}
            
            context_data = None
            full_answer = ""
            
            async for chunk in self.nodes.generate_answer_stream(initial_query_state):
                chunk_type = chunk.get("type")
                
                if chunk_type == "context":
                    # ä¿å­˜ä¸Šä¸‹æ–‡æ•°æ®
                    context_data = chunk.get("context")
                    yield {
                        "type": "context",
                        "context": context_data
                    }
                elif chunk_type == "reasoning_chunk":
                    # è½¬å‘æ€è€ƒæ¨ç†è¿‡ç¨‹
                    yield {
                        "type": "reasoning_chunk",
                        "content": chunk.get("content", ""),
                        "done": chunk.get("done", False),
                        "full_reasoning": chunk.get("full_reasoning", "")
                    }
                elif chunk_type == "answer_chunk":
                    content = chunk.get("content", "")
                    is_done = chunk.get("done", False)
                    
                    if not is_done:
                        full_answer += content
                        yield {
                            "type": "answer_chunk",
                            "content": content
                        }
                    else:
                        # ç­”æ¡ˆç”Ÿæˆå®Œæˆ
                        if "full_answer" in chunk:
                            full_answer = chunk["full_answer"]
                        
                        # æ›´æ–°å¯¹è¯å†å²
                        new_chat_history = (chat_history or []) + [
                            {"role": "user", "content": question},
                            {"role": "assistant", "content": full_answer}
                        ]
                        
                        yield {
                            "type": "complete",
                            "answer": full_answer,
                            "context": context_data or {},
                            "chat_history": new_chat_history
                        }
                        
                        logger.info(f"âœ… æµå¼æŸ¥è¯¢å®Œæˆ (thread_id={thread_id[:8]}...)")
                        
        except Exception as e:
            logger.error(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield {
                "type": "error",
                "content": f"æŸ¥è¯¢å‡ºé”™: {str(e)}"
            }


# --- Main ---
async def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    agent = await RAGAgent.create()
    
    # ç´¢å¼•æ–‡æ¡£
    await agent.index_documents(["data/inputs/111002_tk.md"])
    
    # ç¬¬ä¸€è½®æŸ¥è¯¢
    result1 = await agent.query(
        "è¿™ä»½ä¿é™©æ¡æ¬¾çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?", 
        mode="hybrid",
        enable_rerank=True
    )
    print("\nğŸ¤– ç¬¬ä¸€è½®ç­”æ¡ˆ:", result1["answer"])
    
    # ç¬¬äºŒè½®æŸ¥è¯¢ï¼ˆå¸¦å¯¹è¯å†å²ï¼‰
    result2 = await agent.query(
        "é‚£çŠ¹è±«æœŸæ˜¯å¤šé•¿æ—¶é—´?",
        mode="hybrid", 
        enable_rerank=True,
        chat_history=result1["chat_history"]  # ä¼ å…¥å¯¹è¯å†å²
    )
    print("\nğŸ¤– ç¬¬äºŒè½®ç­”æ¡ˆ:", result2["answer"])


if __name__ == "__main__":
    asyncio.run(main())