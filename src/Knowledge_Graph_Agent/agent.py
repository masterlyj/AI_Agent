import asyncio
import os
from typing import List, Optional
#å¯¼å…¥ Reranker æ¨¡å—
from .reranker import RerankerModel
from .light_graph_rag import LightRAG
from .nodes import WorkflowNodes
from .graph import create_indexing_graph, create_querying_graph
from .state import IndexingState
from .utils import logger
from .kg.shared_storage import initialize_pipeline_status
from .llm import get_llm
from .async_lanchain_rag_adapter import create_lightrag_compatible_complete
from .embedding_factory import get_embedder, create_lightrag_embedding_adapter
from .mineru_integration import SmartDocumentIndexer

# --- RAGAgent with Real LLM and Embedding ---
class RAGAgent:
    def __init__(self):
        self.rag: Optional[LightRAG] = None
        self.nodes: Optional[WorkflowNodes] = None
        self.indexing_graph = None
        self.querying_graph = None
        self.smart_indexer: Optional[SmartDocumentIndexer] = None
        self.reranker: Optional[RerankerModel] = None

    @classmethod
    async def create(cls, working_dir: str = "data/rag_storage", rerank_config: dict = None):
        instance = cls()
        instance.working_dir = working_dir
        os.makedirs(working_dir, exist_ok=True)
        
        # === 1. è·å– LangChain LLM å®ä¾‹ ===
        langchain_llm = get_llm()  # è‡ªåŠ¨é€‰æ‹© DeepSeek / Gemini
        
        # === 2. åŒ…è£…ä¸º LightRAG å…¼å®¹çš„å¼‚æ­¥å‡½æ•° ===
        llm_func = create_lightrag_compatible_complete(
            langchain_llm,
            retry_attempts=3,
            retry_min_wait=4
        )
        
        # === 3. è·å–åµŒå…¥æ¨¡å‹ ===
        # é…ç½® Ollama åµŒå…¥æ¨¡å‹ (qwen3_embedding:0.6b)
        embedding_config = {
            "type": "ollama",
            "model": "qwen3-embedding:0.6b",
            "base_url": "http://localhost:11434"
        }
        
        # åˆ›å»º LangChain åµŒå…¥æ¨¡å‹å®ä¾‹
        langchain_embedder = get_embedder(embedding_config)
        
        # é€‚é…ä¸º LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
        embedding_func = create_lightrag_embedding_adapter(
            langchain_embedder,
            embedding_dim=1024
        )
        #åˆå§‹åŒ– Reranker æ¨¡å‹
        if rerank_config and rerank_config.get("enabled", False):
            logger.info("ğŸ”§ åˆå§‹åŒ– Reranker æ¨¡å‹...")
            try:
                instance.reranker = RerankerModel(
                    model_name_or_path=rerank_config.get("model"),
                    device=rerank_config.get("device")
                )
                logger.info("âœ… Reranker æ¨¡å‹åŠ è½½å®Œæˆã€‚")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ Reranker æ¨¡å‹å¤±è´¥: {e}")
                instance.reranker = None
        
        # === 4. åˆ›å»º LightRAG å®ä¾‹ ===
        instance.rag = LightRAG(
            working_dir=working_dir,
            embedding_func=embedding_func,
            llm_model_func=llm_func,
        )
        
        # === 5. åˆå§‹åŒ–å­˜å‚¨å’Œæµæ°´çº¿ ===
        await instance.rag.initialize_storages()
        await initialize_pipeline_status()
        
        # === 6. åˆå§‹åŒ–å·¥ä½œæµ ===
        instance.nodes = WorkflowNodes(instance.rag)
        instance.indexing_graph = create_indexing_graph(instance.nodes)
        instance.querying_graph = create_querying_graph(instance.nodes)
        
        # === 7. åˆå§‹åŒ–æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨ ===
        # ä»ç¯å¢ƒå˜é‡è·å–MinerU APIå¯†é’¥
        mineru_api_key = os.environ.get("MINERU_API_KEY", "")
        instance.smart_indexer = SmartDocumentIndexer(mineru_api_key=mineru_api_key)
        
        return instance

    async def index_documents(self, file_paths: List[str]):
        """æ™ºèƒ½ç´¢å¼•æ–‡æ¡£ - æ”¯æŒPDFå’Œæ–‡æœ¬æ–‡ä»¶"""
        logger.info(f"ğŸ“š å¼€å§‹æ™ºèƒ½ç´¢å¼• {len(file_paths)} ä¸ªæ–‡æ¡£...")
        
        # ä½¿ç”¨æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨å¤„ç†æ–‡ä»¶
        if self.smart_indexer:
            process_result = await self.smart_indexer.process_files_for_indexing(file_paths)
            files_to_index = process_result["files_to_index"]
            
            if not files_to_index:
                logger.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶")
                return {
                    "track_id": None,
                    "status_message": "æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶",
                    "processing_summary": self.smart_indexer.get_processing_summary(process_result)
                }
            
            logger.info(f"ğŸ“„ å‡†å¤‡ç´¢å¼• {len(files_to_index)} ä¸ªå¤„ç†åçš„æ–‡ä»¶")
        else:
            # å¦‚æœæ²¡æœ‰æ™ºèƒ½ç´¢å¼•å™¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶
            files_to_index = file_paths
        
        # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œç´¢å¼•
        contents, ids, paths = [], [], []
        for fp in files_to_index:
            try:
                with open(fp, 'r', encoding='utf-8') as f:
                    contents.append(f.read())
                ids.append(os.path.basename(fp))
                paths.append(os.path.abspath(fp))
                logger.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {os.path.basename(fp)}")
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {fp}: {e}")
                continue
        
        if not contents:
            logger.error("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶å†…å®¹")
            return {
                "track_id": None,
                "status_message": "æ–‡ä»¶è¯»å–å¤±è´¥",
                "processing_summary": "æ–‡ä»¶è¯»å–å¤±è´¥"
            }

        initial_state: IndexingState = {
            "working_dir": self.working_dir,
            "inputs": contents,
            "ids": ids,
            "file_paths": paths,
            "track_id": None,
            "status_message": ""
        }

        result = await self.indexing_graph.ainvoke(initial_state)
        
        # æ·»åŠ å¤„ç†æ‘˜è¦åˆ°ç»“æœä¸­
        if self.smart_indexer and 'processing_summary' not in result:
            result['processing_summary'] = self.smart_indexer.get_processing_summary(process_result)
        
        logger.info(f"ğŸ“Œ ç´¢å¼•æµç¨‹ç»“æŸ: {result['status_message']}")
        return result

    async def query(self, question: str, mode: str = "hybrid", enable_rerank: bool = True):
        """é€šè¿‡ LangGraph æŸ¥è¯¢æµç¨‹æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (naive, local, global, hybrid)
        
        Returns:
            åŒ…å« context å’Œ answer çš„å­—å…¸
        """
        from .state import QueryState
        
        # æ„é€ åˆå§‹æŸ¥è¯¢çŠ¶æ€
        initial_query_state: QueryState = {
            "working_dir": self.working_dir,
            "query": question,
            "query_mode": mode,
            "reranker": self.reranker if enable_rerank else None,
            "context": {},
            "answer": ""
        }
        
        result = await self.querying_graph.ainvoke(initial_query_state)
        
        logger.info(f"ğŸ” æŸ¥è¯¢æµç¨‹å®Œæˆ (mode={mode})")
        return result


# --- Main ---
async def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    agent = await RAGAgent.create()
    
    # ç´¢å¼•æ–‡æ¡£
    await agent.index_documents(["data/inputs/111002_tk.md"])
    
    # æŸ¥è¯¢
    result = await agent.query("è¿™ä»½ä¿é™©æ¡æ¬¾çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?", mode="hybrid")
    print("\nğŸ¤– ç­”æ¡ˆ:", result)


if __name__ == "__main__":
    asyncio.run(main())