import asyncio
import os
from typing import List, Optional
from .light_graph_rag import LightRAG
from .nodes import WorkflowNodes
from .graph import create_indexing_graph, create_querying_graph
from .state import IndexingState
from .utils import logger
from .kg.shared_storage import initialize_pipeline_status
from .llm import get_llm
from .async_lanchain_rag_adapter import create_lightrag_compatible_complete
from .embedding_factory import get_embedder, create_lightrag_embedding_adapter

# --- RAGAgent with Real LLM and Embedding ---
class RAGAgent:
    def __init__(self):
        self.rag: Optional[LightRAG] = None
        self.nodes: Optional[WorkflowNodes] = None
        self.indexing_graph = None
        self.querying_graph = None

    @classmethod
    async def create(cls, working_dir: str = "data/rag_storage"):
        instance = cls()
        instance.working_dir = working_dir
        os.makedirs(working_dir, exist_ok=True)
        
        # === 1. è·å– LangChain LLM å®ä¾‹ ===
        langchain_llm = get_llm()  # è‡ªåŠ¨é€‰æ‹© DeepSeek / Gemini
        
        # === 2. åŒ…è£…ä¸º LightRAG å…¼å®¹çš„å¼‚æ­¥å‡½æ•° ===
        llm_func = create_lightrag_compatible_complete(
            langchain_llm,
            retry_attempts=3,
            retry_min_wait=4
        )
        
        # === 3. è·å–åµŒå…¥æ¨¡å‹ ===
        # é…ç½® Ollama åµŒå…¥æ¨¡å‹ (qwen3_embedding:0.6b)
        embedding_config = {
            "type": "ollama",
            "model": "qwen3-embedding:0.6b",
            "base_url": "http://localhost:11434"
        }
        
        # åˆ›å»º LangChain åµŒå…¥æ¨¡å‹å®ä¾‹
        langchain_embedder = get_embedder(embedding_config)
        
        # é€‚é…ä¸º LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
        embedding_func = create_lightrag_embedding_adapter(
            langchain_embedder,
            embedding_dim=1024
        )
        
        # === 4. åˆ›å»º LightRAG å®ä¾‹ ===
        instance.rag = LightRAG(
            working_dir=working_dir,
            embedding_func=embedding_func,
            llm_model_func=llm_func,
        )
        
        # === 5. åˆå§‹åŒ–å­˜å‚¨å’Œæµæ°´çº¿ ===
        await instance.rag.initialize_storages()
        await initialize_pipeline_status()
        
        # === 6. åˆå§‹åŒ–å·¥ä½œæµ ===
        instance.nodes = WorkflowNodes(instance.rag)
        instance.indexing_graph = create_indexing_graph(instance.nodes)
        instance.querying_graph = create_querying_graph(instance.nodes)
        
        return instance

    async def index_documents(self, file_paths: List[str]):
        """ç´¢å¼•æ–‡æ¡£"""
        contents, ids, paths = [], [], []
        for fp in file_paths:
            with open(fp, 'r', encoding='utf-8') as f:
                contents.append(f.read())
            ids.append(os.path.basename(fp))
            paths.append(os.path.abspath(fp))

        initial_state: IndexingState = {
            "working_dir": self.working_dir,
            "inputs": contents,
            "ids": ids,
            "file_paths": paths,
            "track_id": None,
            "status_message": ""
        }

        result = await self.indexing_graph.ainvoke(initial_state)
        logger.info(f"ğŸ“Œ ç´¢å¼•æµç¨‹ç»“æŸ: {result['status_message']}")
        return result

    async def query(self, question: str, mode: str = "hybrid"):
        """é€šè¿‡ LangGraph æŸ¥è¯¢æµç¨‹æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (naive, local, global, hybrid)
        
        Returns:
            åŒ…å« context å’Œ answer çš„å­—å…¸
        """
        from .state import QueryState
        
        # æ„é€ åˆå§‹æŸ¥è¯¢çŠ¶æ€
        initial_query_state: QueryState = {
            "working_dir": self.working_dir,
            "query": question,
            "query_mode": mode,
            "context": {},
            "answer": ""
        }
        
        result = await self.querying_graph.ainvoke(initial_query_state)
        
        logger.info(f"ğŸ” æŸ¥è¯¢æµç¨‹å®Œæˆ (mode={mode})")
        return result


# --- Main ---
async def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    agent = await RAGAgent.create()
    
    # ç´¢å¼•æ–‡æ¡£
    await agent.index_documents(["data/inputs/111002_tk.md"])
    
    # æŸ¥è¯¢
    result = await agent.query("è¿™ä»½ä¿é™©æ¡æ¬¾çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?", mode="hybrid")
    print("\nğŸ¤– ç­”æ¡ˆ:", result)


if __name__ == "__main__":
    asyncio.run(main())