import asyncio
import os
from typing import List, Optional, Dict
from dotenv import load_dotenv

#å¯¼å…¥ Reranker æ¨¡å—
from .reranker import RerankerModel
from .light_graph_rag import LightRAG
from .nodes import WorkflowNodes
from .graph import create_indexing_graph, create_querying_graph
from .state import IndexingState
from .utils import logger
from .kg.shared_storage import initialize_pipeline_status
from .llm import get_llm
from .async_lanchain_rag_adapter import create_lightrag_compatible_complete
from .embedding_factory import get_embedder, create_lightrag_embedding_adapter
from .mineru_integration import SmartDocumentIndexer

#åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- RAGAgent with Real LLM and Embedding ---
class RAGAgent:
    def __init__(self):
        self.rag: Optional[LightRAG] = None
        self.nodes: Optional[WorkflowNodes] = None
        self.indexing_graph = None
        self.querying_graph = None
        self.smart_indexer: Optional[SmartDocumentIndexer] = None
        self.reranker: Optional[RerankerModel] = None
        self.langchain_llm = None
        self.rerank_top_k = 20

    @classmethod
    async def create(cls, working_dir: str = "data/rag_storage", rerank_config: dict = None, storage_mode: str = "database"):
        """
        åˆ›å»ºRAGAgentå®ä¾‹
        
        Args:
            working_dir: å·¥ä½œç›®å½•è·¯å¾„
            rerank_config: é‡æ’åºé…ç½®
            storage_mode: å­˜å‚¨æ¨¡å¼ï¼Œå¯é€‰"memory"ï¼ˆå†…å­˜ç®¡ç†ï¼‰æˆ–"database"ï¼ˆæ•°æ®åº“å­˜å‚¨ï¼‰
        """
        instance = cls()
        instance.working_dir = working_dir
        instance.storage_mode = storage_mode  # å­˜å‚¨å­˜å‚¨æ¨¡å¼
        os.makedirs(working_dir, exist_ok=True)
        
        # === 1. è·å– LangChain LLM å®ä¾‹ ===
        langchain_llm = get_llm() 
        
        # LLM å®ä¾‹
        instance.langchain_llm = langchain_llm
        logger.info(f"âœ… LangChain LLM å·²åŠ è½½ç”¨äºé—®ç­”ç”Ÿæˆ")
        
        # === 2. åŒ…è£…ä¸º LightRAG å…¼å®¹çš„å¼‚æ­¥å‡½æ•° ===
        llm_func = create_lightrag_compatible_complete(
            langchain_llm,
            retry_attempts=3,
            retry_min_wait=4
        )
        
        # === 3. è·å–åµŒå…¥æ¨¡å‹ï¼ˆä» .env æ„å»ºé…ç½®ï¼‰ ===
        etype = os.getenv("EMBEDDING_TYPE", "ollama").strip()
        
        if etype == "hf":
            model_name = os.getenv("HF_EMBEDDING_MODEL_NAME", "BAAI/bge-m3").strip()
            model_kwargs = {}
            device = os.getenv("HF_EMBEDDING_DEVICE", "").strip()
            if device:
                model_kwargs["device"] = device
            # å¯é€‰é™„åŠ å‚æ•°
            if os.getenv("HF_EMBEDDING_TRUST_REMOTE_CODE", "false").lower() == "true":
                model_kwargs["trust_remote_code"] = True
            
            embedding_config = {
                "type": "hf",
                "model_name": model_name,
                "model_kwargs": model_kwargs,
                "encode_kwargs": {},
                "show_progress": os.getenv("HF_EMBEDDING_SHOW_PROGRESS", "false").lower() == "true",
                "multi_process": os.getenv("HF_EMBEDDING_MULTI_PROCESS", "false").lower() == "true",
            }
            logger.info(f"âœ… é…ç½® HuggingFace Embedding: {model_name}")
        
        elif etype == "ollama":
            embedding_config = {
                "type": "ollama",
                "model": os.getenv("OLLAMA_EMBEDDING_MODEL", "qwen3-embedding:0.6b").strip(),
                "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").strip(),
            }
            logger.info(f"âœ… é…ç½® Ollama Embedding: {embedding_config['model']}")
        
        elif etype == "vllm":
            base_url = os.getenv("VLLM_BASE_URL")
            if not base_url:
                raise ValueError("EMBEDDING_TYPE=vllm ä½†æœªé…ç½® VLLM_BASE_URL")
            embedding_config = {
                "type": "vllm",
                "model": os.getenv("VLLM_EMBEDDING_MODEL", "text-embedding-3-large").strip(),
                "base_url": base_url.strip(),
                "api_key": os.getenv("VLLM_API_KEY", "EMPTY"),
            }
            logger.info(f"âœ… é…ç½® vLLM Embedding: {embedding_config['model']}")
        else:
            raise ValueError(f"æœªçŸ¥ EMBEDDING_TYPE: {etype}ï¼Œæ”¯æŒ: hf, ollama, vllm")
        
        # åˆ›å»º LangChain åµŒå…¥æ¨¡å‹å®ä¾‹
        langchain_embedder = get_embedder(embedding_config)
        
        # ä» .env è¯»å– embedding ç»´åº¦
        embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))
        
        # é€‚é…ä¸º LightRAG å…¼å®¹çš„åµŒå…¥å‡½æ•°
        embedding_func = create_lightrag_embedding_adapter(
            langchain_embedder,
            embedding_dim=embedding_dim
        )
        
        # === 4. åˆå§‹åŒ– Reranker æ¨¡å‹ï¼ˆ.env ä¸ºé»˜è®¤ï¼Œå…¥å‚å¯è¦†ç›–ï¼‰ ===
        env_enabled = os.getenv("RERANK_ENABLED", "false").lower() == "true"
        cfg = rerank_config or {}
        enabled = cfg.get("enabled", env_enabled)
        
        if enabled:
            logger.info("ğŸ”§ åˆå§‹åŒ– Reranker æ¨¡å‹...")
            try:
                model = cfg.get("model", os.getenv("RERANK_MODEL", "maidalun1020/bce-reranker-base_v1").strip())
                device = cfg.get("device", os.getenv("RERANK_DEVICE", "").strip() or None)
                top_k = int(cfg.get("top_k", os.getenv("RERANK_TOP_K", "20")))
                use_fp16 = cfg.get("use_fp16", os.getenv("RERANK_USE_FP16", "false").lower() == "true")
                
                instance.reranker = RerankerModel(
                    model_name_or_path=model,
                    device=device,
                    top_k=top_k,
                    use_fp16=use_fp16,
                )
                instance.rerank_top_k = top_k
                logger.info(f"âœ… Reranker æ¨¡å‹åŠ è½½å®Œæˆ (model={model}, top_k={instance.rerank_top_k})")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ Reranker æ¨¡å‹å¤±è´¥: {e}")
                instance.reranker = None
        
        # === 5. åˆ›å»º LightRAG å®ä¾‹ ===
        # æ ¹æ®storage_modeå‚æ•°é€‰æ‹©ä¸åŒçš„å­˜å‚¨æ–¹å¼
        if storage_mode == "memory":
            # å†…å­˜ç®¡ç†æ¨¡å¼ - ä½¿ç”¨æœ¬åœ°JSONæ–‡ä»¶å­˜å‚¨
            logger.info("ğŸ“ ä½¿ç”¨å†…å­˜ç®¡ç†æ¨¡å¼ - æœ¬åœ°JSONæ–‡ä»¶å­˜å‚¨")
            instance.rag = LightRAG( 
                working_dir=working_dir,
                embedding_func=embedding_func,
                llm_model_func=llm_func,
                kv_storage="JsonKVStorage",
                vector_storage="NanoVectorDBStorage",
                graph_storage="NetworkXStorage",
                doc_status_storage="JsonDocStatusStorage"
            )
        else:
            # æ•°æ®åº“å­˜å‚¨æ¨¡å¼ - ä½¿ç”¨PostgreSQLå­˜å‚¨ï¼ˆé»˜è®¤ï¼‰
            logger.info("ğŸ—„ï¸ ä½¿ç”¨æ•°æ®åº“å­˜å‚¨æ¨¡å¼ - PostgreSQLå­˜å‚¨")
            instance.rag = LightRAG( 
                working_dir=working_dir,
                embedding_func=embedding_func,
                llm_model_func=llm_func,
                kv_storage="PGKVStorage",
                vector_storage="PGVectorStorage",
                graph_storage="PGGraphStorage",
                doc_status_storage="PGDocStatusStorage"
            )
        
        # === 6. åˆå§‹åŒ–å­˜å‚¨å’Œæµæ°´çº¿ ===
        await instance.rag.initialize_storages()
        await initialize_pipeline_status()
        
        # === 7. åˆå§‹åŒ–å·¥ä½œæµ ===
        instance.nodes = WorkflowNodes(instance.rag)
        instance.indexing_graph = create_indexing_graph(instance.nodes)
        instance.querying_graph = create_querying_graph(instance.nodes)
        
        # === 8. åˆå§‹åŒ–æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨ ===
        # ä»ç¯å¢ƒå˜é‡è·å–MinerU APIå¯†é’¥
        mineru_api_key = os.environ.get("MINERU_API_KEY", "")
        instance.smart_indexer = SmartDocumentIndexer(mineru_api_key=mineru_api_key)
        
        return instance

    async def index_documents(self, file_paths: List[str]):
        """æ™ºèƒ½ç´¢å¼•æ–‡æ¡£ - æ”¯æŒPDFå’Œæ–‡æœ¬æ–‡ä»¶"""
        logger.info(f"ğŸ“š å¼€å§‹æ™ºèƒ½ç´¢å¼• {len(file_paths)} ä¸ªæ–‡æ¡£...")
        
        # ä½¿ç”¨æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å™¨å¤„ç†æ–‡ä»¶
        if self.smart_indexer:
            process_result = await self.smart_indexer.process_files_for_indexing(file_paths)
            files_to_index = process_result["files_to_index"]
            
            if not files_to_index:
                logger.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶")
                return {
                    "track_id": None,
                    "status_message": "æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡ä»¶",
                    "processing_summary": self.smart_indexer.get_processing_summary(process_result)
                }
            
            logger.info(f"ğŸ“„ å‡†å¤‡ç´¢å¼• {len(files_to_index)} ä¸ªå¤„ç†åçš„æ–‡ä»¶")
        else:
            # å¦‚æœæ²¡æœ‰æ™ºèƒ½ç´¢å¼•å™¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶
            files_to_index = file_paths
        
        # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œç´¢å¼•
        contents, ids, paths = [], [], []
        for fp in files_to_index:
            try:
                with open(fp, 'r', encoding='utf-8') as f:
                    contents.append(f.read())
                ids.append(os.path.basename(fp))
                paths.append(os.path.abspath(fp))
                logger.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {os.path.basename(fp)}")
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {fp}: {e}")
                continue
        
        if not contents:
            logger.error("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶å†…å®¹")
            return {
                "track_id": None,
                "status_message": "æ–‡ä»¶è¯»å–å¤±è´¥",
                "processing_summary": "æ–‡ä»¶è¯»å–å¤±è´¥"
            }

        initial_state: IndexingState = {
            "working_dir": self.working_dir,
            "inputs": contents,
            "ids": ids,
            "file_paths": paths,
            "track_id": None,
            "status_message": ""
        }

        result = await self.indexing_graph.ainvoke(initial_state)
        
        # æ·»åŠ å¤„ç†æ‘˜è¦åˆ°ç»“æœä¸­
        if self.smart_indexer and 'processing_summary' not in result:
            result['processing_summary'] = self.smart_indexer.get_processing_summary(process_result)
        
        logger.info(f"ğŸ“Œ ç´¢å¼•æµç¨‹ç»“æŸ: {result['status_message']}")
        return result

    async def query(
        self, 
        question: str, 
        mode: str = "mix", 
        enable_rerank: bool = True,
        rerank_top_k: Optional[int] = None,
        chat_history: List[Dict] = None,
        thread_id: str = None
    ):
        """é€šè¿‡ LangGraph æŸ¥è¯¢æµç¨‹æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (naive, local, global, hybrid, mix)
            enable_rerank: æ˜¯å¦å¯ç”¨ç²¾æ’
            rerank_top_k: ç²¾æ’æ•°é‡
            chat_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            thread_id: ä¼šè¯æ ‡è¯†ï¼ˆå¯é€‰ï¼Œç”¨äºä¼šè¯ç®¡ç†ï¼‰
        
        Returns:
            åŒ…å« context, answer, chat_history çš„å­—å…¸
        """
        from src.Knowledge_Graph_Agent.state import QueryState

        # å¦‚æœæœªæä¾› thread_idï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶ ID
        if thread_id is None:
            import uuid
            thread_id = str(uuid.uuid4())

        initial_query_state: QueryState = {
            "thread_id": thread_id,  # ä¼ å…¥ä¼šè¯æ ‡è¯†
            "working_dir": self.working_dir,
            "query": question,
            "query_mode": mode,
            "llm": self.langchain_llm,
            "reranker": self.reranker if enable_rerank else None,
            "rerank_top_k": self.rerank_top_k,
            "chat_history": chat_history or [],
            "retrieved_docs": [],
            "retrieved_entities": [],
            "retrieved_relationships": [],
            "final_docs": [],
            "context": {},
            "answer": ""
        }

        # é€šè¿‡ config ä¼ é€’ thread_idï¼ˆç”¨äº LangGraph å†…éƒ¨è¿½è¸ªï¼‰
        config = {"configurable": {"thread_id": thread_id}}

        # ainvoke æ˜¯ç‹¬ç«‹æ‰§è¡Œï¼ŒçŠ¶æ€ä¸ä¼šè·¨è°ƒç”¨ä¿ç•™
        result = await self.querying_graph.ainvoke(
            initial_query_state,
            config=config  # å¯ç”¨äºæ£€æŸ¥ç‚¹/æŒä¹…åŒ–
        )

        logger.info(f"ğŸ” æŸ¥è¯¢æµç¨‹å®Œæˆ (thread_id={thread_id[:8]}..., mode={mode})")

        return {
            "answer": result.get("answer", ""),
            "context": result.get("context", {}),
            "chat_history": result.get("chat_history", [])
        }


# --- Main ---
async def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    agent = await RAGAgent.create()
    
    # ç´¢å¼•æ–‡æ¡£
    await agent.index_documents(["data/inputs/111002_tk.md"])
    
    # ç¬¬ä¸€è½®æŸ¥è¯¢
    result1 = await agent.query(
        "è¿™ä»½ä¿é™©æ¡æ¬¾çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ?", 
        mode="hybrid",
        enable_rerank=True
    )
    print("\nğŸ¤– ç¬¬ä¸€è½®ç­”æ¡ˆ:", result1["answer"])
    
    # ç¬¬äºŒè½®æŸ¥è¯¢ï¼ˆå¸¦å¯¹è¯å†å²ï¼‰
    result2 = await agent.query(
        "é‚£çŠ¹è±«æœŸæ˜¯å¤šé•¿æ—¶é—´?",
        mode="hybrid", 
        enable_rerank=True,
        chat_history=result1["chat_history"]  # ä¼ å…¥å¯¹è¯å†å²
    )
    print("\nğŸ¤– ç¬¬äºŒè½®ç­”æ¡ˆ:", result2["answer"])


if __name__ == "__main__":
    asyncio.run(main())